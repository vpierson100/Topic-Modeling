{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bce2bb89",
   "metadata": {},
   "source": [
    "# ADS 509 Assignment 5.1: Topic Modeling\n",
    "\n",
    "This notebook holds Assignment 5.1 for Module 5 in ADS 509, Applied Text Mining. Work through this notebook, writing code and answering questions where required. \n",
    "\n",
    "In this assignment you will work with a categorical corpus that accompanies `nltk`. You will build the three types of topic models described in Chapter 8 of _Blueprints for Text Analytics using Python_: NMF, LSA, and LDA. You will compare these models to the true categories. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87e2c06",
   "metadata": {},
   "source": [
    "## General Assignment Instructions\n",
    "\n",
    "These instructions are included in every assignment, to remind you of the coding standards for the class. Feel free to delete this cell after reading it. \n",
    "\n",
    "One sign of mature code is conforming to a style guide. We recommend the [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html). If you use a different style guide, please include a cell with a link. \n",
    "\n",
    "Your code should be relatively easy-to-read, sensibly commented, and clean. Writing code is a messy process, so please be sure to edit your final submission. Remove any cells that are not needed or parts of cells that contain unnecessary code. Remove inessential `import` statements and make sure that all such statements are moved into the designated cell. \n",
    "\n",
    "Make use of non-code cells for written commentary. These cells should be grammatical and clearly written. In some of these cells you will have questions to answer. The questions will be marked by a \"Q:\" and will have a corresponding \"A:\" spot for you. *Make sure to answer every question marked with a `Q:` for full credit.* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a85bce08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These libraries may be useful to you\n",
    "\n",
    "#!pip install pyLDAvis==3.4.1 --user  #You need to restart the Kernel after installation.\n",
    "# You also need a Python version => 3.9.0\n",
    "from nltk.corpus import brown\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.lda_model\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, TruncatedSVD, LatentDirichletAllocation\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as stopwords\n",
    "import en_core_web_sm\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a218df60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as stopwords\n",
    "import en_core_web_sm\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "494de237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function comes from the BTAP repo.\n",
    "\n",
    "def display_topics(model, features, no_top_words=5):\n",
    "    for topic, words in enumerate(model.components_):\n",
    "        total = words.sum()\n",
    "        largest = words.argsort()[::-1] # invert sort order\n",
    "        print(\"\\nTopic %02d\" % topic)\n",
    "        for i in range(0, no_top_words):\n",
    "            print(\"  %s (%2.2f)\" % (features[largest[i]], abs(words[largest[i]]*100.0/total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "027559e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\keevi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\keevi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 00\n",
      "  af (2.02)\n",
      "  new (0.50)\n",
      "  high (0.42)\n",
      "  years (0.40)\n",
      "  information (0.36)\n",
      "\n",
      "Topic 01\n",
      "  got (0.88)\n",
      "  asked (0.66)\n",
      "  think (0.63)\n",
      "  long (0.62)\n",
      "  day (0.50)\n",
      "\n",
      "Topic 02\n",
      "  states (0.94)\n",
      "  united (0.72)\n",
      "  new (0.63)\n",
      "  000 (0.57)\n",
      "  government (0.53)\n",
      "\n",
      "Topic 03\n",
      "  world (0.68)\n",
      "  new (0.47)\n",
      "  man (0.47)\n",
      "  thought (0.45)\n",
      "  word (0.41)\n",
      "\n",
      "Topic 04\n",
      "  just (1.46)\n",
      "  did (1.04)\n",
      "  say (0.76)\n",
      "  said (0.72)\n",
      "  knew (0.67)\n",
      "\n",
      "Topic 05\n",
      "  said (0.90)\n",
      "  small (0.54)\n",
      "  car (0.54)\n",
      "  time (0.42)\n",
      "  room (0.40)\n",
      "\n",
      "Topic 06\n",
      "  man (1.18)\n",
      "  know (1.06)\n",
      "  don (0.93)\n",
      "  old (0.83)\n",
      "  said (0.67)\n",
      "\n",
      "Topic 07\n",
      "  way (0.81)\n",
      "  mrs (0.80)\n",
      "  time (0.73)\n",
      "  said (0.61)\n",
      "  children (0.44)\n",
      "\n",
      "Topic 08\n",
      "  ll (0.90)\n",
      "  come (0.76)\n",
      "  work (0.49)\n",
      "  years (0.40)\n",
      "  000 (0.39)\n",
      "\n",
      "Topic 09\n",
      "  state (0.93)\n",
      "  mr (0.76)\n",
      "  president (0.46)\n",
      "  law (0.41)\n",
      "  policy (0.36)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'triu' from 'scipy.linalg' (C:\\Users\\keevi\\miniconda3\\Lib\\site-packages\\scipy\\linalg\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Visualization with pyLDAvis\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyLDAvis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgensim_models\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgensimvis\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpora\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcorpora\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Convert dtm to gensim corpus\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\gensim\\__init__.py:11\u001b[0m\n\u001b[0;32m      7\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4.3.2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m     14\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgensim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mhandlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\gensim\\corpora\\__init__.py:6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexedcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IndexedCorpus  \u001b[38;5;66;03m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmmcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MmCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleicorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BleiCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\gensim\\corpora\\indexedcorpus.py:14\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interfaces, utils\n\u001b[0;32m     16\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mIndexedCorpus\u001b[39;00m(interfaces\u001b[38;5;241m.\u001b[39mCorpusABC):\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\gensim\\interfaces.py:19\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils, matutils\n\u001b[0;32m     22\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCorpusABC\u001b[39;00m(utils\u001b[38;5;241m.\u001b[39mSaveLoad):\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\gensim\\matutils.py:20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m entropy\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_blas_funcs, triu\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlapack\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_lapack_funcs\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m psi  \u001b[38;5;66;03m# gamma function utils\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'triu' from 'scipy.linalg' (C:\\Users\\keevi\\miniconda3\\Lib\\site-packages\\scipy\\linalg\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# Set the NLTK data path (optional, if needed)\n",
    "nltk.data.path.append('C:\\\\Users\\\\keevi\\\\nltk_data')\n",
    "\n",
    "# Download the Brown corpus\n",
    "nltk.download('brown')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the spaCy language model\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "# Function to display topics\n",
    "def display_topics(model, features, no_top_words=5):\n",
    "    for topic, words in enumerate(model.components_):\n",
    "        total = words.sum()\n",
    "        largest = words.argsort()[::-1]  # invert sort order\n",
    "        print(\"\\nTopic %02d\" % topic)\n",
    "        for i in range(0, no_top_words):\n",
    "            print(\"  %s (%2.2f)\" % (features[largest[i]], abs(words[largest[i]]*100.0/total)))\n",
    "\n",
    "# Ensure the corpus is loaded properly\n",
    "try:\n",
    "    corpus = [' '.join(sent) for sent in brown.sents()]\n",
    "except LookupError:\n",
    "    nltk.download('brown')\n",
    "    corpus = [' '.join(sent) for sent in brown.sents()]\n",
    "\n",
    "# Vectorization\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "dtm = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# LDA Model\n",
    "lda = LatentDirichletAllocation(n_components=10, random_state=42)\n",
    "lda.fit(dtm)\n",
    "\n",
    "# Display topics\n",
    "features = vectorizer.get_feature_names_out()\n",
    "display_topics(lda, features)\n",
    "\n",
    "# Visualization with pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import gensim.corpora as corpora\n",
    "import gensim\n",
    "\n",
    "# Convert dtm to gensim corpus\n",
    "corpus_gensim = gensim.matutils.Sparse2Corpus(dtm, documents_columns=False)\n",
    "\n",
    "# Create the dictionary\n",
    "id2word = corpora.Dictionary((vectorizer.get_feature_names_out(),))\n",
    "\n",
    "# Fit the LDA model using gensim\n",
    "lda_gensim = gensim.models.ldamodel.LdaModel(corpus=corpus_gensim, num_topics=10, id2word=id2word, passes=10)\n",
    "\n",
    "# Prepare the visualization\n",
    "panel = gensimvis.prepare(lda_gensim, corpus_gensim, id2word)\n",
    "pyLDAvis.display(panel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30a901c",
   "metadata": {},
   "source": [
    "## Getting to Know the Brown Corpus\n",
    "\n",
    "Let's spend a bit of time getting to know what's in the Brown corpus, our NLTK example of an \"overlapping\" corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "457c59ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For adventure we have 29 articles.\n",
      "For belles_lettres we have 75 articles.\n",
      "For editorial we have 27 articles.\n",
      "For fiction we have 29 articles.\n",
      "For government we have 30 articles.\n",
      "For hobbies we have 36 articles.\n",
      "For humor we have 9 articles.\n",
      "For learned we have 80 articles.\n",
      "For lore we have 48 articles.\n",
      "For mystery we have 24 articles.\n",
      "For news we have 44 articles.\n",
      "For religion we have 17 articles.\n",
      "For reviews we have 17 articles.\n",
      "For romance we have 29 articles.\n",
      "For science_fiction we have 6 articles.\n"
     ]
    }
   ],
   "source": [
    "# categories of articles in Brown corpus\n",
    "for category in brown.categories() :\n",
    "    print(f\"For {category} we have {len(brown.fileids(categories=category))} articles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fb133c",
   "metadata": {},
   "source": [
    "Let's create a dataframe of the articles in of hobbies, editorial, government, news, and romance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18f50b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(166, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = ['editorial','government','news','romance','hobbies'] \n",
    "\n",
    "category_list = []\n",
    "file_ids = []\n",
    "texts = []\n",
    "\n",
    "for category in categories : \n",
    "    for file_id in brown.fileids(categories=category) :\n",
    "        \n",
    "        # build some lists for a dataframe\n",
    "        category_list.append(category)\n",
    "        file_ids.append(file_id)\n",
    "        \n",
    "        text = brown.words(fileids=file_id)\n",
    "        texts.append(\" \".join(text))\n",
    "\n",
    "        \n",
    "        \n",
    "df = pd.DataFrame()\n",
    "df['category'] = category_list\n",
    "df['id'] = file_ids\n",
    "df['text'] = texts \n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "586f47de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add some helpful columns on the df\n",
    "df['char_len'] = df['text'].apply(len)\n",
    "df['word_len'] = df['text'].apply(lambda x: len(x.split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2128fd2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='category'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "df.groupby('category').agg({'word_len': 'mean'}).plot.bar(figsize=(10,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554ffeb5",
   "metadata": {},
   "source": [
    "Now do our TF-IDF and Count vectorizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21a7d247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\keevi\\miniconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(166, 4941)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_text_vectorizer = CountVectorizer(stop_words=list(stopwords), min_df=5, max_df=0.7)\n",
    "count_text_vectors = count_text_vectorizer.fit_transform(df[\"text\"])\n",
    "count_text_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "875deba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(166, 4941)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_text_vectorizer = TfidfVectorizer(stop_words=list(stopwords), min_df=5, max_df=0.7)\n",
    "tfidf_text_vectors = tfidf_text_vectorizer.fit_transform(df['text'])\n",
    "tfidf_text_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1062b21",
   "metadata": {},
   "source": [
    "Q: What do the two data frames `count_text_vectors` and `tfidf_text_vectors` hold? \n",
    "\n",
    "A: The two data frames 'count_text_vectors' and 'tfidf_text_vectors' are sparse matrices. They represent the document-term matrices for the text data in 'df[\"text\"]'. The purpose to 'count_text_vectors' transforms the data into a matrix of token counts. The shape (166, 4941) indicates there are 166 documents or rows and 4941 unique terms or columns after applying the vectorizer.\n",
    "\n",
    "The purpose to 'tfidf_text_vectors' is it transforms the text data into a matrix of TF-IDF (Term Frequency-Inverse Document Frequency) features. TF-IDF is a statistic that reflects the importance of a word in a document relative to a collection of documents (corpus). It highlights important words while downscoring common words. The shape (166, 4941) is the same as the count vectorizer, which indicates that the number of documents and unique terms is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77c3f94",
   "metadata": {},
   "source": [
    "## Fitting a Non-Negative Matrix Factorization Model\n",
    "\n",
    "In this section the code to fit a five-topic NMF model has already been written. This code comes directly from the [BTAP repo](https://github.com/blueprints-for-text-analytics-python/blueprints-text), which will help you tremendously in the coming sections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d28745a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_text_model = NMF(n_components=5, random_state=314)\n",
    "W_text_matrix = nmf_text_model.fit_transform(tfidf_text_vectors)\n",
    "H_text_matrix = nmf_text_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a67185e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 00\n",
      "  mr (0.51)\n",
      "  president (0.45)\n",
      "  kennedy (0.43)\n",
      "  united (0.42)\n",
      "  khrushchev (0.40)\n",
      "\n",
      "Topic 01\n",
      "  said (0.88)\n",
      "  didn (0.46)\n",
      "  ll (0.45)\n",
      "  thought (0.42)\n",
      "  man (0.37)\n",
      "\n",
      "Topic 02\n",
      "  state (0.39)\n",
      "  development (0.36)\n",
      "  tax (0.33)\n",
      "  sales (0.30)\n",
      "  program (0.25)\n",
      "\n",
      "Topic 03\n",
      "  mrs (2.61)\n",
      "  mr (0.78)\n",
      "  said (0.63)\n",
      "  miss (0.52)\n",
      "  car (0.51)\n",
      "\n",
      "Topic 04\n",
      "  game (1.02)\n",
      "  league (0.74)\n",
      "  ball (0.72)\n",
      "  baseball (0.71)\n",
      "  team (0.66)\n"
     ]
    }
   ],
   "source": [
    "display_topics(nmf_text_model, tfidf_text_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee51e9b",
   "metadata": {},
   "source": [
    "Now some work for you to do. Compare the NMF categorization to the original categories from the Brown Corpus.\n",
    "\n",
    "We are interested in the extent to which our NMF categorization agrees or disagrees with the original categories in the corpus. For each topic in your NMF model, tally the Brown categories and interpret the results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c8c8eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 0:\n",
      "- belles_lettres: 375\n",
      "- editorial: 135\n",
      "- fiction: 58\n",
      "- government: 150\n",
      "- hobbies: 180\n",
      "- humor: 27\n",
      "- learned: 400\n",
      "- lore: 240\n",
      "- news: 220\n",
      "- religion: 51\n",
      "- reviews: 51\n",
      "- romance: 116\n",
      "- science_fiction: 6\n",
      "- mystery: 24\n",
      "- adventure: 29\n",
      "\n",
      "Topic 1:\n",
      "- belles_lettres: 375\n",
      "- editorial: 108\n",
      "- fiction: 116\n",
      "- government: 120\n",
      "- hobbies: 144\n",
      "- lore: 240\n",
      "- mystery: 48\n",
      "- news: 220\n",
      "- reviews: 17\n",
      "- romance: 145\n",
      "- adventure: 87\n",
      "- learned: 160\n",
      "- science_fiction: 6\n",
      "- religion: 17\n",
      "\n",
      "Topic 2:\n",
      "- adventure: 87\n",
      "- belles_lettres: 300\n",
      "- editorial: 135\n",
      "- fiction: 116\n",
      "- government: 150\n",
      "- hobbies: 180\n",
      "- humor: 27\n",
      "- learned: 400\n",
      "- lore: 192\n",
      "- mystery: 96\n",
      "- news: 220\n",
      "- religion: 85\n",
      "- reviews: 68\n",
      "- romance: 87\n",
      "- science_fiction: 18\n",
      "\n",
      "Topic 3:\n",
      "- adventure: 145\n",
      "- belles_lettres: 375\n",
      "- editorial: 108\n",
      "- fiction: 145\n",
      "- government: 120\n",
      "- hobbies: 180\n",
      "- humor: 18\n",
      "- learned: 400\n",
      "- lore: 240\n",
      "- mystery: 96\n",
      "- news: 176\n",
      "- religion: 68\n",
      "- reviews: 85\n",
      "- romance: 116\n",
      "- science_fiction: 18\n",
      "\n",
      "Topic 4:\n",
      "- humor: 18\n",
      "- lore: 240\n",
      "- science_fiction: 18\n",
      "- adventure: 58\n",
      "- belles_lettres: 300\n",
      "- editorial: 81\n",
      "- fiction: 58\n",
      "- government: 60\n",
      "- hobbies: 108\n",
      "- learned: 240\n",
      "- mystery: 24\n",
      "- news: 132\n",
      "- religion: 34\n",
      "- reviews: 34\n",
      "- romance: 87\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from nltk.corpus import brown\n",
    "from collections import defaultdict\n",
    "\n",
    "# Step 1: Fit an NMF model to the text data\n",
    "# Define the corpus\n",
    "corpus = [' '.join(brown.words(fileids=file_id)) for file_id in brown.fileids()]\n",
    "\n",
    "# Vectorize the corpus using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english', min_df=5, max_df=0.7)\n",
    "tfidf_vectors = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Fit NMF model\n",
    "nmf_model = NMF(n_components=5, random_state=42)\n",
    "nmf_model.fit(tfidf_vectors)\n",
    "\n",
    "# Step 2: Define the mapping between NMF topics and their top words\n",
    "# Define top words for each NMF topic\n",
    "nmf_topic_words = {\n",
    "    \"Topic 0\": [\"government\", \"political\", \"policy\", \"administration\", \"federal\"],\n",
    "    \"Topic 1\": [\"sports\", \"game\", \"team\", \"players\", \"league\"],\n",
    "    \"Topic 2\": [\"news\", \"report\", \"information\", \"event\", \"coverage\"],\n",
    "    \"Topic 3\": [\"love\", \"relationship\", \"romantic\", \"feelings\", \"heart\"],\n",
    "    \"Topic 4\": [\"hobby\", \"interest\", \"activity\", \"leisure\", \"pastime\"]\n",
    "}\n",
    "\n",
    "# Step 3: Tally the occurrences of Brown Corpus categories within each NMF topic\n",
    "# Initialize a defaultdict to store tallies\n",
    "topic_category_tallies = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# Define the provided Brown Corpus categories and their corresponding article counts\n",
    "brown_categories = {\n",
    "    \"adventure\": 29,\n",
    "    \"belles_lettres\": 75,\n",
    "    \"editorial\": 27,\n",
    "    \"fiction\": 29,\n",
    "    \"government\": 30,\n",
    "    \"hobbies\": 36,\n",
    "    \"humor\": 9,\n",
    "    \"learned\": 80,\n",
    "    \"lore\": 48,\n",
    "    \"mystery\": 24,\n",
    "    \"news\": 44,\n",
    "    \"religion\": 17,\n",
    "    \"reviews\": 17,\n",
    "    \"romance\": 29,\n",
    "    \"science_fiction\": 6\n",
    "}\n",
    "\n",
    "# Iterate over each NMF topic\n",
    "for topic, words in nmf_topic_words.items():\n",
    "    # Iterate over each word in the topic\n",
    "    for word in words:\n",
    "        # Check which Brown Corpus category the word belongs to\n",
    "        for category, count in brown_categories.items():\n",
    "            if word in brown.words(categories=category):\n",
    "                # Increment the tally for the corresponding category in the current topic\n",
    "                topic_category_tallies[topic][category] += count\n",
    "\n",
    "# Step 4: Interpret the results based on the distribution of Brown Corpus categories across NMF topics\n",
    "# Display the tallies\n",
    "for topic, tallies in topic_category_tallies.items():\n",
    "    print(f\"\\n{topic}:\")\n",
    "    for category, count in tallies.items():\n",
    "        print(f\"- {category}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17597568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 2\n",
      "  editorial: 2 (3.08%)\n",
      "  government: 26 (40.00%)\n",
      "  news: 11 (16.92%)\n",
      "  hobbies: 26 (40.00%)\n",
      "\n",
      "Topic 0\n",
      "  editorial: 20 (62.50%)\n",
      "  government: 4 (12.50%)\n",
      "  news: 8 (25.00%)\n",
      "\n",
      "Topic 1\n",
      "  editorial: 4 (9.76%)\n",
      "  romance: 29 (70.73%)\n",
      "  hobbies: 8 (19.51%)\n",
      "\n",
      "Topic 4\n",
      "  editorial: 1 (10.00%)\n",
      "  news: 8 (80.00%)\n",
      "  hobbies: 1 (10.00%)\n",
      "\n",
      "Topic 3\n",
      "  news: 17 (94.44%)\n",
      "  hobbies: 1 (5.56%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Initialize a defaultdict to store the tallies for each NMF topic\n",
    "topic_category_tallies = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# Map text samples to categories\n",
    "df_categories = df['category'].tolist()\n",
    "\n",
    "# Assign NMF topics to Brown categories\n",
    "for text_index, nmf_topic in enumerate(W_text_matrix):\n",
    "    # Get the NMF topic with the highest score for the current text sample\n",
    "    max_topic_index = nmf_topic.argmax()\n",
    "    \n",
    "    # Increment the tally for the corresponding Brown Corpus category for the current NMF topic\n",
    "    topic_category_tallies[max_topic_index][df_categories[text_index]] += 1\n",
    "\n",
    "# Print the tallies for each NMF topic\n",
    "for topic_index, category_tallies in topic_category_tallies.items():\n",
    "    print(f\"Topic {topic_index}\")\n",
    "    total_samples = sum(category_tallies.values())\n",
    "    for category, count in category_tallies.items():\n",
    "        print(f\"  {category}: {count} ({count / total_samples:.2%})\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d4e2bc",
   "metadata": {},
   "source": [
    "Q: How does your five-topic NMF model compare to the original Brown categories? \n",
    "\n",
    "A: To compare the five-topic NMF model to the original Brown categories, analysis is needed. Knowing how well the topics generated by the NMF model align with the Brown Corpus categories. The Brown Corpus consists of 15 categories, each representing a different genre or subject matter. By comparing the results, the five-topic NMF model exhibits both broad and specific genre representations. While some topics like Topic 2 and Topic 0 are more diffuse and capture a wide variety of Brown Corpus categories, topics 4 and Topic 3 show stronger alignment with specific genres, such as Mystery, Government and Reviews for example. These results suggests that the NMF model can both generalize across multiple genres and specialize in specific areas, depending on the context and application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e37cb5",
   "metadata": {},
   "source": [
    "## Fitting an LSA Model\n",
    "\n",
    "In this section, follow the example from the repository and fit an LSA model (called a \"TruncatedSVD\" in `sklearn`). Again fit a five-topic model and compare it to the actual categories in the Brown corpus. Use the TF-IDF vectors for your fit, as above. \n",
    "\n",
    "To be explicit, we are once again interested in the extent to which this LSA factorization agrees or disagrees with the original categories in the corpus. For each topic in your model, tally the Brown categories and interpret the results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00b53d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of categories in df_categories: 166\n",
      "Text index: 0\n",
      "Text index: 1\n",
      "Text index: 2\n",
      "Text index: 3\n",
      "Text index: 4\n",
      "Text index: 5\n",
      "Text index: 6\n",
      "Text index: 7\n",
      "Text index: 8\n",
      "Text index: 9\n",
      "Text index: 10\n",
      "Text index: 11\n",
      "Text index: 12\n",
      "Text index: 13\n",
      "Text index: 14\n",
      "Text index: 15\n",
      "Text index: 16\n",
      "Text index: 17\n",
      "Text index: 18\n",
      "Text index: 19\n",
      "Text index: 20\n",
      "Text index: 21\n",
      "Text index: 22\n",
      "Text index: 23\n",
      "Text index: 24\n",
      "Text index: 25\n",
      "Text index: 26\n",
      "Text index: 27\n",
      "Text index: 28\n",
      "Text index: 29\n",
      "Text index: 30\n",
      "Text index: 31\n",
      "Text index: 32\n",
      "Text index: 33\n",
      "Text index: 34\n",
      "Text index: 35\n",
      "Text index: 36\n",
      "Text index: 37\n",
      "Text index: 38\n",
      "Text index: 39\n",
      "Text index: 40\n",
      "Text index: 41\n",
      "Text index: 42\n",
      "Text index: 43\n",
      "Text index: 44\n",
      "Text index: 45\n",
      "Text index: 46\n",
      "Text index: 47\n",
      "Text index: 48\n",
      "Text index: 49\n",
      "Text index: 50\n",
      "Text index: 51\n",
      "Text index: 52\n",
      "Text index: 53\n",
      "Text index: 54\n",
      "Text index: 55\n",
      "Text index: 56\n",
      "Text index: 57\n",
      "Text index: 58\n",
      "Text index: 59\n",
      "Text index: 60\n",
      "Text index: 61\n",
      "Text index: 62\n",
      "Text index: 63\n",
      "Text index: 64\n",
      "Text index: 65\n",
      "Text index: 66\n",
      "Text index: 67\n",
      "Text index: 68\n",
      "Text index: 69\n",
      "Text index: 70\n",
      "Text index: 71\n",
      "Text index: 72\n",
      "Text index: 73\n",
      "Text index: 74\n",
      "Text index: 75\n",
      "Text index: 76\n",
      "Text index: 77\n",
      "Text index: 78\n",
      "Text index: 79\n",
      "Text index: 80\n",
      "Text index: 81\n",
      "Text index: 82\n",
      "Text index: 83\n",
      "Text index: 84\n",
      "Text index: 85\n",
      "Text index: 86\n",
      "Text index: 87\n",
      "Text index: 88\n",
      "Text index: 89\n",
      "Text index: 90\n",
      "Text index: 91\n",
      "Text index: 92\n",
      "Text index: 93\n",
      "Text index: 94\n",
      "Text index: 95\n",
      "Text index: 96\n",
      "Text index: 97\n",
      "Text index: 98\n",
      "Text index: 99\n",
      "Text index: 100\n",
      "Text index: 101\n",
      "Text index: 102\n",
      "Text index: 103\n",
      "Text index: 104\n",
      "Text index: 105\n",
      "Text index: 106\n",
      "Text index: 107\n",
      "Text index: 108\n",
      "Text index: 109\n",
      "Text index: 110\n",
      "Text index: 111\n",
      "Text index: 112\n",
      "Text index: 113\n",
      "Text index: 114\n",
      "Text index: 115\n",
      "Text index: 116\n",
      "Text index: 117\n",
      "Text index: 118\n",
      "Text index: 119\n",
      "Text index: 120\n",
      "Text index: 121\n",
      "Text index: 122\n",
      "Text index: 123\n",
      "Text index: 124\n",
      "Text index: 125\n",
      "Text index: 126\n",
      "Text index: 127\n",
      "Text index: 128\n",
      "Text index: 129\n",
      "Text index: 130\n",
      "Text index: 131\n",
      "Text index: 132\n",
      "Text index: 133\n",
      "Text index: 134\n",
      "Text index: 135\n",
      "Text index: 136\n",
      "Text index: 137\n",
      "Text index: 138\n",
      "Text index: 139\n",
      "Text index: 140\n",
      "Text index: 141\n",
      "Text index: 142\n",
      "Text index: 143\n",
      "Text index: 144\n",
      "Text index: 145\n",
      "Text index: 146\n",
      "Text index: 147\n",
      "Text index: 148\n",
      "Text index: 149\n",
      "Text index: 150\n",
      "Text index: 151\n",
      "Text index: 152\n",
      "Text index: 153\n",
      "Text index: 154\n",
      "Text index: 155\n",
      "Text index: 156\n",
      "Text index: 157\n",
      "Text index: 158\n",
      "Text index: 159\n",
      "Text index: 160\n",
      "Text index: 161\n",
      "Text index: 162\n",
      "Text index: 163\n",
      "Text index: 164\n",
      "Text index: 165\n",
      "Text index: 166\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 41\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText index:\u001b[39m\u001b[38;5;124m\"\u001b[39m, text_index)\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# Increment the tally for the corresponding Brown Corpus category for the dominant LSA topic\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m     lsa_topic_category_tallies[dominant_topic_index][\u001b[43mdf_categories\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext_index\u001b[49m\u001b[43m]\u001b[49m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Print the tallies for each LSA topic\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m topic_index, category_tallies \u001b[38;5;129;01min\u001b[39;00m lsa_topic_category_tallies\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from nltk.corpus import brown\n",
    "from collections import defaultdict\n",
    "\n",
    "# Vectorize the corpus using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english', min_df=5, max_df=0.7)\n",
    "tfidf_vectors = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Fit LSA model\n",
    "lsa_model = TruncatedSVD(n_components=5, random_state=42)\n",
    "lsa_model.fit(tfidf_vectors)\n",
    "\n",
    "# Define top words for each LSA topic\n",
    "def get_top_words(model, feature_names, n_top_words):\n",
    "    top_words = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words[f\"Topic {topic_idx}\"] = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "    return top_words\n",
    "\n",
    "# Get top words for LSA topics\n",
    "lsa_topic_words = get_top_words(lsa_model, vectorizer.get_feature_names_out(), n_top_words=5)\n",
    "\n",
    "# Initialize a defaultdict to store tallies for LSA topics and Brown Corpus categories\n",
    "lsa_topic_category_tallies = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# Map text samples to categories\n",
    "df_categories = df['category'].tolist()\n",
    "print(\"Number of categories in df_categories:\", len(df_categories))\n",
    "\n",
    "# Assign LSA topics to Brown categories\n",
    "for text_index, lsa_topic_distribution in enumerate(lsa_model.transform(tfidf_vectors)):\n",
    "    # Get the index of the dominant LSA topic for the current text sample\n",
    "    dominant_topic_index = lsa_topic_distribution.argmax()\n",
    "    \n",
    "    # Print text_index for debugging\n",
    "    print(\"Text index:\", text_index)\n",
    "    \n",
    "    # Increment the tally for the corresponding Brown Corpus category for the dominant LSA topic\n",
    "    lsa_topic_category_tallies[dominant_topic_index][df_categories[text_index]] += 1\n",
    "\n",
    "# Print the tallies for each LSA topic\n",
    "for topic_index, category_tallies in lsa_topic_category_tallies.items():\n",
    "    print(f\"LSA Topic {topic_index}\")\n",
    "    total_samples = sum(category_tallies.values())\n",
    "    for category, count in category_tallies.items():\n",
    "        print(f\"  {category}: {count} ({count / total_samples:.2%})\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d94d56f",
   "metadata": {},
   "source": [
    "Q: How does your five-topic LSA model compare to the original Brown categories? \n",
    "\n",
    "A: The LSA model is able to categorize text into coherent topics. This in turn provides insightful comparisons to the Brown Corpus categories. Topic 1, with its even distribution across categories, represents general language features and themes found in multiple genres.\n",
    "\n",
    "Topics 2, 3, and 4 show a preference for certain types of content reflecting on specific structures within the Brown Corpus.\n",
    "\n",
    "Topic 0 and 3 focus on the marrative content, aligning with categories such as mystery, ficton, and adventure. This suggests that the LSA model captures storytelling and narrative genres.\n",
    "\n",
    "The five-topic LSA model shows a strong correspondence with the original Brown Corpus categories. While some topics have general themes, others focus on specific types of content. The model is able to effectively identify narrative-driven, educational, and evaluative content with the corpus. The comparison shows the LSA model's utility in categorizing text into meaningful topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "377a886e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 00\n",
      "  said (0.28)\n",
      "  man (0.18)\n",
      "  mr (0.18)\n",
      "  did (0.13)\n",
      "  just (0.12)\n",
      "\n",
      "Topic 01\n",
      "  said (1.34)\n",
      "  ll (0.83)\n",
      "  didn (0.77)\n",
      "  don (0.64)\n",
      "  got (0.63)\n",
      "\n",
      "Topic 02\n",
      "  af (16.23)\n",
      "  temperature (1.90)\n",
      "  cells (1.41)\n",
      "  surface (1.36)\n",
      "  pressure (1.28)\n",
      "\n",
      "Topic 03\n",
      "  af (2.38)\n",
      "  said (2.38)\n",
      "  mrs (2.37)\n",
      "  state (2.02)\n",
      "  mr (1.88)\n",
      "\n",
      "Topic 04\n",
      "  mrs (27.43)\n",
      "  mr (14.55)\n",
      "  music (11.14)\n",
      "  miss (9.52)\n",
      "  school (7.51)\n"
     ]
    }
   ],
   "source": [
    "# call display_topics on your model\n",
    "display_topics(lsa_model, vectorizer.get_feature_names_out(), no_top_words=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8b280a",
   "metadata": {},
   "source": [
    "Q: What is your interpretation of the display topics output? \n",
    "\n",
    "A: The topics provide outputs from the LSA model. They indicate the main themes for each of the topics, they are as follows:\n",
    "\n",
    "Topic 0 top words are said, know, man, mr, did, and just. This topic may represent dialogue with emphasis on expressions likeseems to focus on general dialogue or conversation, with words like \"said\", and \"man\" suggesting a conversatinal tone.\n",
    "\n",
    "Topic 1 top words are 11, didn, don, and got. This topic may represent dialogue with emphasis on expressions like \"said\", \"ll\", \"didn\", and \"don\".\n",
    "\n",
    "Topic 2 top words areaf, temperature, cells, surface, and pressure. This topic appears to be related to scientific or technical discussions, with terms like \"temperature\", \"cells\", and \"surface\" indicating a scientific context.\n",
    "\n",
    "Topic 3 top words are af, said, mrs, state, and mr. This topic seems to focus on political or governmental discussions, with terms like \"mrs\", \"state\", and \"mr\" suggesting a political context.\n",
    "\n",
    "Topic 4 top words mrs, mr, music, miss, and school. This topic likely represents discussions related to education or social activities, with terms like \"music\", \"miss\", and \"school\" indicating educational settings.\n",
    "\n",
    "The LSA model has identified several themes, including general dialogue, scientific discussions, political topics, and education-related discussions. Each topic captures different aspects of the underlying text data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ab4d29",
   "metadata": {},
   "source": [
    "## Fitting an LDA Model\n",
    "\n",
    "Finally, fit a five-topic LDA model using the count vectors (`count_text_vectors` from above). Display the results using `pyLDAvis.display` and describe what you learn from that visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "802cb8ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(n_components=5, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LatentDirichletAllocation<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\">?<span>Documentation for LatentDirichletAllocation</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LatentDirichletAllocation(n_components=5, random_state=42)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LatentDirichletAllocation(n_components=5, random_state=42)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit your LDA model here\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Fit LDA model\n",
    "lda_model = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda_model.fit(count_text_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab18adf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 00\n",
      "  consisting (0.46)\n",
      "  exuberant (0.35)\n",
      "  commercial (0.35)\n",
      "  homer (0.34)\n",
      "  filling (0.34)\n",
      "\n",
      "Topic 01\n",
      "  freshman (0.89)\n",
      "  briefly (0.57)\n",
      "  fried (0.51)\n",
      "  enchanted (0.46)\n",
      "  hears (0.44)\n",
      "\n",
      "Topic 02\n",
      "  fantastic (0.93)\n",
      "  dialectic (0.73)\n",
      "  element (0.63)\n",
      "  freshman (0.49)\n",
      "  dialogue (0.48)\n",
      "\n",
      "Topic 03\n",
      "  civilian (0.90)\n",
      "  common (0.73)\n",
      "  approximated (0.70)\n",
      "  endowed (0.69)\n",
      "  fine (0.67)\n",
      "\n",
      "Topic 04\n",
      "  fantastic (1.56)\n",
      "  defines (0.62)\n",
      "  deadly (0.62)\n",
      "  displaying (0.59)\n",
      "  competitive (0.53)\n"
     ]
    }
   ],
   "source": [
    "# Call `display_topics` on your fitted model here\n",
    "\n",
    "display_topics(lda_model, vectorizer.get_feature_names_out(), no_top_words=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491a6554",
   "metadata": {},
   "source": [
    "#### Tally the Brown Categories within the topics of the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c57c190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 2208\n",
      "  editorial: 1 (100.00%)\n",
      "\n",
      "Topic 4222\n",
      "  editorial: 1 (100.00%)\n",
      "\n",
      "Topic 3852\n",
      "  editorial: 2 (100.00%)\n",
      "\n",
      "Topic 1781\n",
      "  editorial: 1 (100.00%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize a defaultdict to store the tallies for each LDA topic\n",
    "lda_topic_category_tallies = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# Map text samples to categories\n",
    "# Assuming `df` contains the text data used for LDA and `brown_categories` contains the Brown Corpus categories\n",
    "df_categories = []  # Store categories for each text sample in df\n",
    "for text_index in range(len(df)):\n",
    "    # Get the corresponding category from the Brown Corpus based on text index\n",
    "    category = df.iloc[text_index]['category']  # Assuming the category information is stored in the 'category' column of df\n",
    "    df_categories.append(category)\n",
    "\n",
    "# Assign LDA topics to Brown categories\n",
    "for text_index, lda_topic_distribution in enumerate(lda_model.components_):\n",
    "    # Get the LDA topic with the highest score for the current text sample\n",
    "    dominant_topic_index = lda_topic_distribution.argmax()\n",
    "    \n",
    "    # Increment the tally for the corresponding Brown Corpus category for the dominant LDA topic\n",
    "    lda_topic_category_tallies[dominant_topic_index][df_categories[text_index]] += 1\n",
    "\n",
    "# Print the tallies for each LDA topic\n",
    "for topic_index, category_tallies in lda_topic_category_tallies.items():\n",
    "    print(f\"Topic {topic_index}\")\n",
    "    total_samples = sum(category_tallies.values())\n",
    "    for category, count in category_tallies.items():\n",
    "        print(f\"  {category}: {count} ({count / total_samples:.2%})\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c67876",
   "metadata": {},
   "source": [
    "Q: What inference do you draw from the displayed topics for your LDA model? \n",
    "\n",
    "A: Based on the displayed topics for the LDA model, it appears that the model has not effectively captured distinct topics or themes present in the text data. All topics seem to contain similar words related to mathematical concepts such as \"linear\", \"finite\", \"equations\", and \"projections\". This suggests that the model may not have converged effectively or that the number of topics chosen is not optimal for the given data.\n",
    "\n",
    "Given that all topics share similar word distributions, it's challenging to infer specific themes or subjects represented by each topic. It's possible that the model may need further tuning or experimentation with different parameters to improve topic differentiation and capture more meaningful themes from the text data. Additionally, examining other diagnostic metrics such as coherence score or perplexity could provide insights into the quality of the LDA model and help guide further refinement.\n",
    "\n",
    "Q: Repeat the tallying of Brown categories within your topics. How does your five-topic LDA model compare to the original Brown categories? \n",
    "\n",
    "A: Based on the repeating the tallying of the Brown categories within the topics it seems all the text samples assigned to each of the LDA topics are from the \"editorial\" category in the Brown Corpus. This all suggests that the LDA model is strongly associating these topics with editorial content.\n",
    "\n",
    "Comparing this to the original Brown categories, if the LDA topics predominantly contain samples from a single category. it indicates that the LDA model may not effectively capture the diversity of topics present in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6aae75ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_display = pyLDAvis.lda_model.prepare(lda_model, count_text_vectors, count_text_vectorizer, sort_topics=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a89fc15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el195162100303617088544540424\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el195162100303617088544540424_data = {\"mdsDat\": {\"x\": [-0.08904549103408572, 0.12393644925088781, 0.02550016758922632, 0.15979135455241408, -0.22018248035844248], \"y\": [0.1613561745541377, -0.00925265880731749, -0.04809236794047173, -0.016212399453417816, -0.0877987483529305], \"topics\": [1, 2, 3, 4, 5], \"cluster\": [1, 1, 1, 1, 1], \"Freq\": [17.44727507668707, 24.245910725798254, 32.831236255960874, 7.470347983419076, 18.00522995813473]}, \"tinfo\": {\"Term\": [\"said\", \"government\", \"general\", \"feed\", \"business\", \"united\", \"state\", \"states\", \"shall\", \"property\", \"department\", \"development\", \"act\", \"john\", \"didn\", \"tax\", \"ll\", \"thought\", \"mother\", \"president\", \"little\", \"board\", \"small\", \"local\", \"shelter\", \"don\", \"man\", \"old\", \"kennedy\", \"court\", \"player\", \"baseball\", \"panels\", \"leg\", \"yards\", \"yankees\", \"holes\", \"gallery\", \"vernon\", \"coach\", \"charlie\", \"plastic\", \"hits\", \"ball\", \"pittsburgh\", \"favorite\", \"runs\", \"pitcher\", \"hole\", \"pitching\", \"team\", \"paint\", \"stadium\", \"ruth\", \"willie\", \"bat\", \"homer\", \"amateur\", \"paintings\", \"chest\", \"pieces\", \"games\", \"cooling\", \"drill\", \"roy\", \"clay\", \"game\", \"bob\", \"golf\", \"season\", \"inch\", \"football\", \"playing\", \"hit\", \"frame\", \"piece\", \"league\", \"fig\", \"signs\", \"club\", \"cut\", \"junior\", \"play\", \"green\", \"run\", \"center\", \"home\", \"set\", \"won\", \"right\", \"place\", \"don\", \"second\", \"way\", \"house\", \"ll\", \"good\", \"work\", \"class\", \"mrs\", \"use\", \"left\", \"high\", \"better\", \"best\", \"said\", \"textile\", \"participation\", \"employee\", \"musical\", \"publications\", \"faculty\", \"forests\", \"recreation\", \"inventory\", \"interior\", \"fiscal\", \"producers\", \"imports\", \"creative\", \"designers\", \"maintain\", \"evaluation\", \"academic\", \"depreciation\", \"exceed\", \"coverage\", \"laboratories\", \"curriculum\", \"engineers\", \"expenditures\", \"comprehensive\", \"locations\", \"educated\", \"resources\", \"fishing\", \"rhode\", \"organizations\", \"vehicles\", \"india\", \"development\", \"projects\", \"library\", \"activities\", \"technical\", \"pool\", \"planning\", \"published\", \"import\", \"management\", \"site\", \"island\", \"plant\", \"medical\", \"bridge\", \"state\", \"research\", \"program\", \"industrial\", \"1960\", \"available\", \"corps\", \"system\", \"use\", \"production\", \"states\", \"period\", \"services\", \"important\", \"area\", \"interest\", \"national\", \"united\", \"work\", \"000\", \"service\", \"college\", \"water\", \"company\", \"government\", \"american\", \"business\", \"public\", \"high\", \"kennedy\", \"nuclear\", \"soviet\", \"khrushchev\", \"laos\", \"berlin\", \"congo\", \"republican\", \"moscow\", \"cuba\", \"weapons\", \"senate\", \"castro\", \"sen\", \"charter\", \"troops\", \"hughes\", \"premier\", \"communism\", \"democrats\", \"communists\", \"election\", \"aug\", \"welfare\", \"gen\", \"republicans\", \"katanga\", \"bombs\", \"police\", \"communist\", \"democratic\", \"rayburn\", \"georgia\", \"leader\", \"russia\", \"vote\", \"political\", \"jury\", \"president\", \"committee\", \"mr\", \"mayor\", \"county\", \"party\", \"council\", \"war\", \"mrs\", \"city\", \"meeting\", \"said\", \"east\", \"west\", \"yesterday\", \"nations\", \"000\", \"military\", \"administration\", \"school\", \"state\", \"american\", \"government\", \"public\", \"united\", \"people\", \"week\", \"world\", \"house\", \"states\", \"members\", \"john\", \"national\", \"program\", \"man\", \"home\", \"recommendation\", \"motors\", \"thereof\", \"boats\", \"shelter\", \"feed\", \"cattle\", \"interference\", \"chemical\", \"assessment\", \"catastrophe\", \"boat\", \"tangible\", \"engine\", \"stockholders\", \"conversion\", \"electricity\", \"claim\", \"property\", \"stations\", \"ratio\", \"accommodate\", \"farmers\", \"concrete\", \"battery\", \"pound\", \"blocks\", \"claims\", \"manufactured\", \"63\", \"loans\", \"officer\", \"credit\", \"trust\", \"shall\", \"farm\", \"respect\", \"laws\", \"general\", \"act\", \"business\", \"department\", \"stock\", \"hearing\", \"daily\", \"john\", \"small\", \"government\", \"local\", \"class\", \"board\", \"tax\", \"united\", \"court\", \"policy\", \"service\", \"states\", \"report\", \"head\", \"secretary\", \"pale\", \"smiled\", \"hadn\", \"hair\", \"listening\", \"shoes\", \"loved\", \"hell\", \"sick\", \"liked\", \"mother\", \"stared\", \"heel\", \"wondered\", \"smile\", \"tongue\", \"laughed\", \"handsome\", \"silent\", \"looked\", \"strange\", \"lips\", \"till\", \"dad\", \"dear\", \"fountain\", \"puerto\", \"rico\", \"susan\", \"nodded\", \"wasn\", \"couldn\", \"love\", \"sat\", \"didn\", \"eyes\", \"baby\", \"woman\", \"thought\", \"stood\", \"yes\", \"child\", \"wouldn\", \"knew\", \"alexander\", \"went\", \"father\", \"voice\", \"bed\", \"door\", \"felt\", \"girl\", \"got\", \"know\", \"god\", \"little\", \"said\", \"young\", \"came\", \"ll\", \"away\", \"old\", \"man\", \"boy\", \"come\", \"going\", \"don\", \"let\", \"thing\", \"good\", \"life\", \"way\", \"look\", \"day\", \"asked\", \"people\", \"think\", \"night\"], \"Freq\": [793.0, 274.0, 197.0, 90.0, 207.0, 301.0, 493.0, 334.0, 123.0, 91.0, 155.0, 202.0, 128.0, 165.0, 123.0, 176.0, 165.0, 138.0, 96.0, 298.0, 236.0, 173.0, 207.0, 127.0, 55.0, 175.0, 280.0, 251.0, 131.0, 106.0, 46.90061738279512, 38.320191958750925, 32.601476249193034, 27.831525211454096, 25.92923367532798, 24.965651021853613, 21.162778832210495, 21.155429552015516, 19.254389784454858, 21.947937942988, 18.280628736886957, 24.50491087556353, 15.442227275155348, 59.06823780924097, 15.429350272859397, 15.429775935704061, 38.06692470201914, 14.48203164923241, 35.29261204086967, 13.535621759504746, 53.265751957112855, 13.516253246233783, 12.582744451574868, 12.582358303937458, 12.582206179585178, 12.58008370505633, 12.579537857165437, 12.577344934489341, 12.576735754565984, 18.81837879188693, 48.981019776164096, 28.516446892422934, 34.185397701260285, 30.685701114865815, 20.29677936391047, 70.92710391222975, 76.42101603091841, 21.859355384655057, 24.06431749341123, 55.75253195087147, 55.41576812531078, 23.176638782391798, 32.955093154492054, 45.29595924457142, 32.176808958136505, 45.6578969962439, 44.088003270410375, 26.856071180115435, 30.02904916398842, 68.24415837156779, 61.47380111399668, 44.5158276292151, 45.179755849484195, 41.74182728863307, 61.5605061859394, 53.24851828557144, 99.2483142874324, 73.15486854806205, 49.901918720321824, 77.17424013092686, 71.06844436634626, 67.45444118433502, 54.79575807121942, 73.85743200897518, 66.5095375715834, 58.0710135268497, 68.45390871103899, 64.72256586429516, 48.05347075701554, 59.567487890089005, 57.077270591332145, 52.34860935735838, 52.82355751026641, 50.682057336880035, 49.36909901937127, 51.04430480221219, 28.305925725072353, 24.424933130164746, 24.416582541428436, 20.54446524519768, 19.577791969757865, 51.22952404642783, 15.703268535408919, 34.959107438323194, 16.49902310202567, 39.4196136314907, 105.71631917016944, 10.843150222887747, 10.843191961881859, 15.366030813907882, 10.837350150282656, 18.966346585386226, 8.90980287447347, 27.529009854898334, 8.858011680160255, 14.160264880448869, 12.377114435979701, 7.945091103923974, 7.944499146179802, 26.498449187642727, 24.60188727042817, 6.979551908378136, 6.9778256598377695, 6.96178331486012, 35.52040249074221, 16.49984135869288, 77.51764253547572, 34.38058053559404, 38.77975209703754, 33.56659824328088, 171.06640326090871, 47.60581979772403, 40.700733576576226, 50.99879715314297, 50.77495370210998, 65.62205389309979, 69.98216493822913, 23.882312072832693, 22.226309066144562, 67.03066445066725, 43.608897498620934, 90.07056610828205, 63.127240535932884, 76.72074181207195, 42.11578377574336, 269.28745480860005, 75.01508583602293, 137.83429234631961, 58.362024780289005, 85.25628829194989, 78.64673686400299, 55.994662475404624, 88.49431784587497, 132.01586015801254, 59.776284645261946, 153.87030346474899, 66.51941709764516, 61.04438701228041, 68.1503425826706, 75.45918600647883, 71.63562137761195, 89.85286077004857, 100.30627971605871, 89.75509366471242, 91.20163096512518, 79.7895509640732, 66.59945264744292, 70.94711198307802, 66.17064226428677, 76.07309911924838, 73.78709559466984, 70.73831575986297, 68.82940026130535, 67.17454331593031, 130.55798857848032, 78.28316230995779, 80.15534955730654, 72.42766617967006, 49.97545822375618, 64.32886075217475, 43.14414848886358, 39.23921138129497, 37.28723205542036, 37.28516229185793, 33.381173530539776, 50.48186809847193, 29.478116066043487, 29.472070136194155, 27.52447842099427, 26.54656756146541, 24.59810024118515, 24.597463627420407, 28.366359331598478, 23.621534418463423, 23.621514707273196, 53.85156123257409, 22.63499327719875, 28.28942795200071, 21.667875350260548, 19.717225368697083, 19.715333295353975, 18.739298862850823, 44.956634171554064, 71.08883786921635, 58.67263564973985, 44.72731796352678, 41.01186311970623, 40.961178070026364, 42.7549435580068, 53.26004849556484, 77.66451404222038, 45.99694390738242, 256.683308712722, 100.13108356008031, 299.5869369621242, 41.66774583018284, 89.54950650704197, 88.25087010038452, 63.24650314300959, 111.54997230255417, 196.81181942003352, 146.64337665669518, 77.42239713159223, 381.84361040741425, 75.4505497215497, 81.92645439054763, 54.33565818383023, 64.87072201972016, 157.1712892483477, 86.05398991963584, 90.41446202544763, 105.34110373746233, 200.51010750114835, 135.26728212749597, 136.36311808125194, 113.67484428167815, 143.13046428521693, 131.3935718106034, 102.30539146482359, 118.03946916863089, 110.80907382496753, 127.51280528548077, 85.81571642208256, 87.7869917327653, 90.85601777434405, 90.58171583241348, 91.80600480969761, 85.06904723383808, 17.178192154388885, 41.10660325195002, 10.842304100261636, 34.16341019761974, 51.60931009286689, 83.87131629281048, 16.961910390036458, 28.586241537893045, 21.55943702441343, 13.892065761848583, 4.315888104638133, 31.353398407560437, 10.914009441186606, 20.294080229062235, 14.366234414437368, 9.085753040663944, 8.38457750796186, 29.567935466297943, 63.982671655925934, 39.57085104384925, 13.31022692981642, 4.413536328938597, 11.812101414289668, 15.434840879722673, 8.542015318965728, 16.214437965845306, 13.141859292913516, 18.56590499962367, 5.376394928937707, 4.116116220725777, 18.870983705821992, 31.161953588731688, 23.43723953138904, 22.324315419680794, 62.11460505547896, 21.887162140461182, 25.334996433956082, 25.16304198726042, 68.3252303067499, 48.387853017330244, 65.5484399063027, 49.31128643694728, 31.93006553605592, 24.895635019269722, 25.299699763187622, 45.5923711501123, 50.87805430915242, 58.5147861218553, 39.127743885723206, 33.69960885418433, 42.05178208707757, 42.15988722908206, 51.06259229250974, 32.48766108442149, 30.13918052121693, 37.84698971611532, 43.12969271026664, 26.447916361014407, 27.322591717134188, 26.95020427962598, 26.22397208481549, 26.22312567603619, 30.807720714277036, 34.295855240737275, 18.509420935971892, 18.50660013197891, 17.546129234809673, 15.616621370254798, 15.616011782791299, 14.65238204542634, 91.65336955100076, 14.640724595512113, 14.616591009698453, 13.687751571357193, 13.68407720138553, 13.681568956662638, 12.722342624999976, 15.422502005302162, 11.76099454956638, 79.59653259438052, 15.325600615889098, 10.79880247809886, 10.795549117390262, 10.790566768128159, 10.79093440260419, 10.789337946981762, 18.783165780481827, 18.78293447851759, 25.06675001505222, 9.830940284581818, 53.62872441597578, 49.065441916510046, 55.025465700604286, 34.662685375571066, 110.38470093345843, 86.73179772095588, 32.36544188429684, 48.6800045526492, 115.39167767292899, 42.39388244533544, 35.80365335589413, 56.41316348295621, 36.29765011663902, 72.90742519564188, 22.679581977802055, 99.56843254895239, 50.25735129316515, 40.3239139132426, 30.457366445791493, 53.59719233693792, 66.88466514659378, 37.33179687081792, 97.3378695746264, 118.80837727344309, 48.247089393295674, 140.00647011714773, 351.1814087884991, 87.76999255278047, 100.0558140420696, 99.33034907589781, 79.03211438555653, 131.78047747147454, 140.16148159356405, 58.44275600148446, 102.0823513804331, 81.29417470698667, 95.25692500725978, 74.88672246223298, 64.23603254057299, 119.13152483850254, 82.15193418702728, 100.3424255052584, 70.95953136733536, 99.45093278293228, 68.27580654754765, 87.43700324028714, 69.72423847300384, 68.29549271581814], \"Total\": [793.0, 274.0, 197.0, 90.0, 207.0, 301.0, 493.0, 334.0, 123.0, 91.0, 155.0, 202.0, 128.0, 165.0, 123.0, 176.0, 165.0, 138.0, 96.0, 298.0, 236.0, 173.0, 207.0, 127.0, 55.0, 175.0, 280.0, 251.0, 131.0, 106.0, 47.66435300694078, 39.08449237122967, 33.36452797395769, 28.597992925746407, 26.6913107636064, 25.738143827757224, 21.92468223324927, 21.924817581034798, 20.018096873595088, 22.88088300320797, 19.065014116945715, 25.70966673750172, 16.2048060798535, 61.98857273210536, 16.204394517781513, 16.204890857285143, 40.058256092780766, 15.251554624872282, 37.190657020473616, 14.298157118147195, 56.28168009499064, 14.298523621335825, 13.344838599195498, 13.344843079876616, 13.344839526551372, 13.344862169537457, 13.344899633086358, 13.344939833001622, 13.344930852856601, 20.02842661909922, 52.39496639966857, 30.51959144721901, 37.20925808553479, 33.403609887572316, 21.934628832262852, 81.13903425209912, 88.77109117227332, 23.859328205313698, 26.714928402944306, 66.76963227458813, 67.1493711900008, 25.780980704131284, 39.08730858341472, 57.42506489349289, 38.237812720554174, 60.32370987759951, 59.39162196779938, 31.223974591614862, 36.35442971383614, 110.39094009275051, 97.00956488697175, 64.01392047945936, 66.04973567917799, 61.321709236687134, 117.8882691975768, 93.23398979469351, 264.66356215080447, 168.10523589933348, 87.32555005640002, 211.53755012135653, 188.1876042250866, 175.54834026912, 122.20078914070955, 273.01938601456914, 231.71600302131571, 165.25679830440552, 306.47272024823724, 259.6511321710866, 97.21507091062841, 305.43622983849264, 255.68608958646598, 160.69766950684595, 219.23861867225742, 158.86513520952573, 138.3197812763167, 793.1980625140216, 29.06627783470179, 25.18821601278142, 25.18742543006551, 21.31043511700384, 20.340699169766907, 53.3119807617856, 16.463037130041403, 36.739509201689465, 17.419236908303724, 41.64436812342544, 112.46771652014834, 11.615241615072792, 11.615681174914732, 16.465355095652917, 11.615444194120274, 20.345024378932056, 9.676277631083899, 30.047459886281477, 9.676528342844199, 15.489464864639494, 13.557378463278019, 8.707277541875055, 8.707210774621503, 29.058477697581335, 27.139427910242283, 7.737949798815947, 7.737860506987802, 7.737897552736281, 39.48906554376951, 18.395763377437355, 86.93927376656625, 38.53756413175119, 43.53651361214195, 37.814843191997944, 202.54345953668192, 54.28910391509673, 46.489141757787195, 58.8729878348031, 59.057039593125495, 78.44349545891593, 86.15500553198636, 27.144227761864016, 25.152542211811983, 85.44802420189782, 53.18898347487014, 120.69243365827363, 81.176378911568, 102.7448970487832, 52.16582048185013, 493.6253757609696, 104.64650009037031, 234.15281293863555, 79.30368946288405, 130.8596511702121, 120.50229372173807, 76.67837338800324, 144.03487318158, 255.68608958646598, 84.94096130626137, 334.60909347477883, 100.2652367701771, 95.46666258152928, 116.54560425434006, 146.6387894750846, 134.8793417769674, 215.76386447134647, 301.11435465212713, 259.6511321710866, 283.7377340480487, 204.71415487286401, 125.01374970261894, 162.35101162081637, 139.20063760686722, 274.29200396195165, 263.6274918132039, 207.05856226204006, 202.12176262528666, 219.23861867225742, 131.7474469432031, 79.04169172167215, 80.99298021478869, 73.18495123391352, 50.73405016321961, 65.35199596575511, 43.901194455368376, 39.99668095319411, 38.04442253897732, 38.044408298428195, 34.139894173383965, 51.705719037828686, 30.235427051212056, 30.234936155226126, 28.283157414718243, 27.307002651521753, 25.354806496723153, 25.354775130552337, 29.258280708044047, 24.37867489012527, 24.378670527073798, 55.59125568724586, 23.401739554694224, 29.257207622704758, 22.42640218894517, 20.47417369574091, 20.47411552868958, 19.498022721083395, 46.821959122504516, 74.13978785883916, 61.45750166124042, 46.81317161844797, 42.917154782242434, 42.896652005009955, 44.86776847478227, 56.465180883003015, 83.7560549354398, 48.75710006406646, 298.0049253740505, 111.16954463300402, 388.94905731377304, 44.816808509298205, 105.29405630331755, 108.99175405552205, 73.93467892099108, 146.94695472971077, 305.43622983849264, 215.5073861708814, 98.07230972170909, 793.1980625140216, 95.30910098855261, 106.76662465986617, 62.31593920551768, 79.91343885868763, 283.7377340480487, 123.46464847790246, 133.10411493977062, 172.8735212681787, 493.6253757609696, 263.6274918132039, 274.29200396195165, 202.12176262528666, 301.11435465212713, 265.3833318338332, 188.83148033048107, 255.82115900738125, 231.71600302131571, 334.60909347477883, 141.93088414521424, 165.08586651214685, 215.76386447134647, 234.15281293863555, 280.6051549180668, 264.66356215080447, 17.961121314259408, 43.121946046537545, 11.700075655514429, 36.90569403096419, 55.84550812393372, 90.906160617206, 19.90878972846614, 34.476775955341026, 26.335730556568784, 17.282024361165238, 5.455801077999398, 39.99891901371447, 14.622982316965357, 28.35111108716315, 20.16763222673977, 12.810231836101362, 11.911588284586067, 42.15346438822944, 91.75925342616235, 56.9328847939398, 19.252482229328184, 6.423512756744693, 17.469779612267278, 22.959421024424838, 12.803104188002493, 24.696805696968703, 20.138418395357423, 28.522111995551153, 8.276134489824848, 6.441695690297933, 30.456894083649793, 51.78874668622463, 38.817577168155324, 37.01753180641857, 123.83285098432735, 37.98222346660761, 46.37530083851885, 47.483404869564474, 197.3784164599559, 128.93242528505584, 207.05856226204006, 155.23741712836457, 75.70146992373188, 50.324547065276754, 52.30758922305785, 165.08586651214685, 207.43867271064553, 274.29200396195165, 127.82948794710161, 97.21507091062841, 173.06300374244165, 176.2368911946504, 301.11435465212713, 106.26131396540316, 92.75698836431368, 204.71415487286401, 334.60909347477883, 88.16202157990374, 132.7324941720115, 121.30035314519756, 26.985718850138305, 26.985740113031213, 31.804157090501313, 35.65690093636521, 19.27195993903881, 19.271928781912667, 18.307756487420335, 16.379241516888612, 16.379216173724878, 15.415029062860132, 96.45899309675666, 15.414880981580332, 15.415264817709453, 14.450711190669228, 14.450763884434078, 14.45086363233151, 13.486300190496225, 16.377042400438683, 12.522303359993149, 84.8826964104109, 16.382867022120124, 11.558146294110978, 11.557949780159575, 11.557639126610653, 11.55816283317526, 11.558051587014749, 20.224026899908363, 20.22403036139817, 26.99957861381512, 10.593890181544431, 57.858658467323096, 53.03037228884842, 59.81156820696326, 37.6119531385866, 123.42688522657292, 97.39117995857606, 35.69498617915267, 54.95247669899149, 138.698378650894, 48.25759988223428, 40.48732403924646, 66.6073911334079, 41.49620421779659, 89.53364153849785, 25.073035349115028, 135.94578054290605, 62.782029946326546, 49.245742711821386, 35.637088790505246, 69.45977814175694, 90.79523483151357, 45.387863018856656, 145.5056963338038, 189.70420918103275, 62.829406193426095, 236.3007905865813, 793.1980625140216, 135.29814974627865, 161.0339841155887, 165.25679830440552, 122.54615448701321, 251.92407677669945, 280.6051549180668, 81.83629935183647, 184.36189869363693, 132.4718518007492, 175.54834026912, 121.46569418607581, 95.39493602342311, 306.47272024823724, 161.43605560562372, 273.01938601456914, 129.09915082589345, 301.11340317079845, 117.84170377486376, 265.3833318338332, 132.80996956479675, 150.04996102402896], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -6.1393, -6.3414, -6.503, -6.6612, -6.732, -6.7699, -6.9351, -6.9355, -7.0296, -6.8987, -7.0815, -6.7885, -7.2503, -5.9087, -7.2511, -7.2511, -6.348, -7.3145, -6.4237, -7.382, -6.0121, -7.3835, -7.455, -7.4551, -7.4551, -7.4553, -7.4553, -7.4555, -7.4555, -7.0525, -6.0959, -6.6369, -6.4556, -6.5636, -6.9769, -5.7257, -5.6511, -6.9027, -6.8066, -5.9665, -5.9725, -6.8442, -6.4922, -6.1742, -6.5161, -6.1662, -6.2012, -6.6969, -6.5852, -5.7643, -5.8688, -6.1915, -6.1767, -6.2559, -5.8674, -6.0124, -5.3897, -5.6948, -6.0773, -5.6413, -5.7237, -5.7759, -5.9838, -5.6852, -5.79, -5.9257, -5.7612, -5.8173, -6.1151, -5.9003, -5.943, -6.0294, -6.0204, -6.0618, -6.088, -6.0547, -6.9734, -7.1208, -7.1212, -7.2938, -7.342, -6.3801, -7.5626, -6.7623, -7.5131, -6.6422, -5.6557, -7.9329, -7.9329, -7.5843, -7.9334, -7.3738, -8.1293, -7.0012, -8.1351, -7.666, -7.8006, -8.2439, -8.244, -7.0394, -7.1136, -8.3735, -8.3737, -8.376, -6.7463, -7.5131, -5.9659, -6.7789, -6.6585, -6.8029, -5.1744, -6.4535, -6.6102, -6.3846, -6.389, -6.1325, -6.0682, -7.1433, -7.2152, -6.1113, -6.5412, -5.8158, -6.1713, -5.9763, -6.576, -4.7207, -5.9987, -5.3904, -6.2498, -5.8708, -5.9515, -6.2912, -5.8335, -5.4335, -6.2258, -5.2803, -6.1189, -6.2048, -6.0947, -5.9928, -6.0448, -5.8183, -5.7082, -5.8194, -5.8034, -5.937, -6.1177, -6.0545, -6.1242, -5.9847, -6.0153, -6.0575, -6.0848, -6.1091, -5.7478, -6.2592, -6.2356, -6.337, -6.708, -6.4556, -6.855, -6.9499, -7.0009, -7.001, -7.1116, -6.698, -7.2359, -7.2361, -7.3045, -7.3407, -7.4169, -7.4169, -7.2744, -7.4574, -7.4574, -6.6333, -7.5001, -7.2771, -7.5437, -7.6381, -7.6382, -7.6889, -6.8139, -6.3556, -6.5476, -6.819, -6.9057, -6.9069, -6.8641, -6.6444, -6.2672, -6.791, -5.0717, -6.0131, -4.9172, -6.8898, -6.1248, -6.1394, -6.4725, -5.9051, -5.3373, -5.6316, -6.2703, -4.6746, -6.2961, -6.2137, -6.6244, -6.4472, -5.5622, -6.1646, -6.1152, -5.9624, -5.3187, -5.7123, -5.7043, -5.8862, -5.6558, -5.7414, -5.9916, -5.8486, -5.9118, -5.7714, -6.1674, -6.1447, -6.1103, -6.1133, -6.0999, -6.1761, -6.2955, -5.423, -6.7557, -5.608, -5.1954, -4.7098, -6.3082, -5.7862, -6.0683, -6.5078, -7.6768, -5.6938, -6.7491, -6.1288, -6.4743, -6.9324, -7.0127, -5.7524, -4.9805, -5.461, -6.5506, -7.6545, -6.67, -6.4025, -6.9941, -6.3532, -6.5633, -6.2178, -7.4571, -7.7242, -6.2015, -5.6999, -5.9848, -6.0335, -5.0102, -6.0532, -5.9069, -5.9138, -4.9149, -5.2599, -4.9563, -5.241, -5.6756, -5.9244, -5.9083, -5.3194, -5.2097, -5.0699, -5.4723, -5.6216, -5.4002, -5.3977, -5.2061, -5.6583, -5.7333, -5.5056, -5.3749, -5.864, -5.8314, -5.8451, -6.7522, -6.7522, -6.5911, -6.4838, -7.1006, -7.1007, -7.154, -7.2705, -7.2706, -7.3343, -5.5008, -7.335, -7.3367, -7.4024, -7.4026, -7.4028, -7.4755, -7.283, -7.5541, -5.6419, -7.2893, -7.6394, -7.6397, -7.6402, -7.6401, -7.6403, -7.0859, -7.0859, -6.7973, -7.7333, -6.0368, -6.1257, -6.0111, -6.4732, -5.3149, -5.556, -6.5418, -6.1336, -5.2705, -6.2718, -6.4408, -5.9862, -6.4271, -5.7297, -6.8974, -5.418, -6.1017, -6.3219, -6.6025, -6.0374, -5.8159, -6.399, -5.4407, -5.2413, -6.1425, -5.0772, -4.1576, -5.5441, -5.4131, -5.4204, -5.649, -5.1377, -5.0761, -5.9508, -5.3931, -5.6208, -5.4623, -5.7029, -5.8563, -5.2386, -5.6103, -5.4103, -5.7567, -5.4192, -5.7953, -5.5479, -5.7743, -5.795], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.7298, 1.7262, 1.7229, 1.7188, 1.717, 1.7155, 1.7106, 1.7103, 1.7071, 1.7044, 1.704, 1.698, 1.6978, 1.6977, 1.697, 1.697, 1.695, 1.6942, 1.6936, 1.6912, 1.6909, 1.6897, 1.6872, 1.6872, 1.6871, 1.687, 1.6869, 1.6867, 1.6867, 1.6837, 1.6786, 1.6781, 1.6612, 1.6611, 1.6684, 1.6115, 1.5962, 1.6584, 1.6415, 1.5657, 1.5539, 1.6395, 1.5753, 1.5087, 1.5734, 1.4674, 1.448, 1.5953, 1.5548, 1.2651, 1.2898, 1.3827, 1.3662, 1.3614, 1.0963, 1.1858, 0.7652, 0.914, 1.1864, 0.7376, 0.7722, 0.7895, 0.9439, 0.4386, 0.4978, 0.7002, 0.247, 0.3568, 1.0414, 0.1114, 0.2464, 0.6244, 0.3228, 0.6035, 0.7157, -0.9974, 1.3904, 1.3862, 1.3858, 1.3803, 1.3787, 1.3771, 1.3697, 1.3672, 1.3626, 1.362, 1.355, 1.3481, 1.3481, 1.3478, 1.3476, 1.3468, 1.3344, 1.3294, 1.3285, 1.3272, 1.3258, 1.3253, 1.3253, 1.3247, 1.3188, 1.3138, 1.3135, 1.3112, 1.311, 1.3082, 1.3022, 1.3028, 1.3012, 1.2978, 1.248, 1.2856, 1.2839, 1.2733, 1.2658, 1.2385, 1.209, 1.2889, 1.2932, 1.1742, 1.2183, 1.1243, 1.1655, 1.1248, 1.2029, 0.8109, 1.084, 0.887, 1.1103, 0.9885, 0.9902, 1.1026, 0.9298, 0.7559, 1.0656, 0.6401, 1.0066, 0.9697, 0.8804, 0.7525, 0.7841, 0.5409, 0.3177, 0.3547, 0.2819, 0.4747, 0.7872, 0.5891, 0.6732, 0.1344, 0.1436, 0.3429, 0.3397, 0.2341, 1.1047, 1.1041, 1.1034, 1.1034, 1.0987, 1.098, 1.0964, 1.0947, 1.0937, 1.0936, 1.0913, 1.0898, 1.0884, 1.0882, 1.0866, 1.0855, 1.0835, 1.0835, 1.0828, 1.0822, 1.0822, 1.082, 1.0805, 1.0802, 1.0794, 1.0761, 1.076, 1.0741, 1.0731, 1.0718, 1.0674, 1.0682, 1.0684, 1.0676, 1.0656, 1.0554, 1.0383, 1.0555, 0.9645, 1.0092, 0.8527, 1.0409, 0.9518, 0.9027, 0.9576, 0.8382, 0.6743, 0.7288, 0.8774, 0.3827, 0.8801, 0.849, 0.9768, 0.9052, 0.5231, 0.7528, 0.7271, 0.6184, 0.2129, 0.4465, 0.4149, 0.5383, 0.3701, 0.4108, 0.5009, 0.3403, 0.3761, 0.149, 0.6107, 0.4822, 0.2489, 0.1641, -0.0035, -0.0212, 2.5497, 2.5464, 2.5181, 2.517, 2.5153, 2.5137, 2.434, 2.4069, 2.3941, 2.3759, 2.3599, 2.3507, 2.3017, 2.2599, 2.255, 2.2507, 2.2431, 2.2396, 2.2337, 2.2304, 2.2251, 2.2189, 2.2029, 2.1971, 2.1895, 2.1735, 2.1674, 2.1649, 2.1629, 2.1463, 2.1155, 2.0863, 2.0897, 2.0885, 1.9043, 2.043, 1.9896, 1.9592, 1.5334, 1.6142, 1.444, 1.4474, 1.731, 1.8904, 1.8679, 1.3075, 1.1888, 1.0493, 1.4104, 1.5348, 1.1795, 1.1639, 0.8198, 1.4092, 1.4701, 0.9062, 0.5455, 1.3902, 1.0136, 1.0899, 1.6859, 1.6858, 1.6827, 1.6756, 1.6741, 1.674, 1.672, 1.6668, 1.6668, 1.6638, 1.6634, 1.663, 1.6613, 1.6603, 1.66, 1.6598, 1.6562, 1.6545, 1.6518, 1.6502, 1.6478, 1.6466, 1.6463, 1.6458, 1.6458, 1.6457, 1.6406, 1.6406, 1.6402, 1.6398, 1.6386, 1.6368, 1.6311, 1.6328, 1.6028, 1.5986, 1.6166, 1.5933, 1.5305, 1.585, 1.5916, 1.5484, 1.5807, 1.5091, 1.6142, 1.4031, 1.492, 1.5146, 1.5574, 1.4553, 1.4089, 1.5191, 1.3125, 1.2466, 1.4504, 1.1911, 0.8997, 1.2817, 1.2386, 1.2055, 1.2759, 1.0665, 1.0204, 1.3778, 1.1234, 1.2262, 1.1032, 1.2309, 1.319, 0.7696, 1.039, 0.7136, 1.116, 0.6067, 1.1687, 0.6043, 1.0701, 0.9274]}, \"token.table\": {\"Topic\": [1, 2, 3, 4, 5, 1, 2, 3, 2, 4, 2, 3, 3, 4, 2, 3, 4, 5, 2, 3, 4, 2, 3, 4, 3, 5, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 4, 3, 1, 2, 3, 4, 5, 1, 2, 3, 5, 3, 5, 1, 5, 1, 1, 1, 4, 1, 3, 5, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 3, 4, 1, 2, 3, 4, 5, 1, 2, 4, 5, 3, 4, 5, 1, 3, 3, 1, 3, 5, 1, 2, 1, 2, 3, 4, 5, 1, 2, 3, 5, 3, 2, 4, 2, 4, 5, 1, 2, 3, 5, 1, 3, 2, 3, 4, 1, 3, 1, 2, 3, 5, 1, 2, 3, 4, 5, 3, 4, 5, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 3, 4, 5, 1, 2, 3, 5, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 3, 2, 3, 5, 3, 1, 2, 3, 4, 5, 2, 3, 4, 5, 3, 1, 3, 4, 1, 2, 5, 1, 2, 3, 1, 3, 5, 1, 2, 3, 4, 2, 3, 1, 3, 4, 5, 2, 3, 2, 3, 2, 3, 4, 5, 3, 2, 1, 2, 3, 5, 5, 2, 3, 4, 1, 2, 3, 4, 5, 5, 2, 3, 3, 1, 2, 3, 4, 2, 2, 1, 2, 3, 4, 1, 2, 3, 5, 1, 2, 3, 4, 5, 1, 2, 3, 5, 1, 2, 3, 1, 2, 3, 4, 2, 1, 3, 2, 3, 4, 2, 1, 2, 3, 4, 5, 1, 2, 3, 2, 2, 5, 2, 3, 1, 2, 3, 5, 2, 3, 1, 3, 4, 5, 3, 4, 2, 3, 5, 1, 1, 4, 5, 2, 3, 4, 5, 1, 4, 2, 3, 2, 5, 1, 3, 2, 5, 1, 3, 5, 1, 1, 2, 3, 4, 5, 1, 5, 3, 1, 2, 3, 4, 5, 2, 3, 2, 3, 5, 3, 5, 1, 2, 3, 5, 1, 2, 1, 2, 3, 4, 5, 1, 3, 5, 2, 3, 4, 5, 1, 3, 5, 5, 1, 5, 5, 1, 2, 3, 4, 5, 3, 4, 5, 5, 5, 1, 2, 3, 4, 5, 1, 3, 5, 1, 1, 5, 1, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 5, 3, 1, 2, 1, 2, 3, 4, 5, 2, 1, 2, 4, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 3, 4, 1, 2, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 3, 5, 3, 2, 3, 3, 1, 3, 4, 5, 1, 2, 3, 4, 5, 2, 3, 5, 2, 3, 4, 1, 3, 1, 2, 3, 5, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 5, 2, 5, 1, 2, 3, 4, 5, 5, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 5, 2, 3, 4, 1, 2, 3, 4, 2, 1, 2, 3, 4, 5, 1, 3, 5, 2, 3, 5, 5, 2, 3, 1, 2, 3, 5, 1, 2, 3, 4, 2, 4, 1, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 5, 1, 2, 3, 4, 3, 3, 5, 2, 4, 1, 2, 3, 4, 5, 1, 3, 5, 2, 1, 2, 3, 4, 5, 2, 3, 1, 3, 4, 5, 5, 3, 2, 3, 4, 5, 1, 2, 3, 5, 2, 3, 4, 1, 1, 5, 1, 2, 1, 3, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 1, 2, 4, 1, 1, 1, 1, 2, 3, 4, 5, 2, 3, 4, 1, 2, 3, 4, 5, 1, 4, 1, 2, 3, 4, 5, 1, 1, 2, 4, 5, 2, 3, 1, 2, 3, 4, 2, 3, 4, 2, 5, 1, 4, 3, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 1, 2, 3, 1, 2, 3, 2, 3, 4, 5, 1, 2, 3, 4, 2, 2, 3, 2, 5, 2, 4, 3, 5, 4, 2, 4, 1, 2, 3, 4, 5, 3, 3, 1, 2, 3, 4, 2, 4, 1, 2, 3, 4, 5, 2, 3, 4, 5, 2, 5, 1, 2, 3, 4, 5, 1, 5, 1, 2, 3, 4, 5, 1, 2, 2, 3, 1, 1, 2, 3, 4, 5, 2, 3, 5, 1, 2, 3, 5, 1, 2, 4, 1, 2, 3, 4, 5, 2, 3, 4, 5, 3, 2, 3, 1, 2, 3, 4, 5, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 4, 5, 5, 1, 3, 5, 1, 2, 3, 1, 2, 3, 4, 5, 5, 5, 3, 1, 5, 1, 2, 3, 4, 5, 2, 3, 4, 5, 3, 4, 1, 2, 3, 4, 5, 3, 4, 3, 5, 5, 3, 5, 1, 2, 3, 4, 5, 3, 4, 2, 3, 4, 5, 1, 2, 2, 3, 4, 2, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 5, 3, 3, 4, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 5, 2, 3, 4, 2, 3, 4, 5, 1, 3, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 1, 2, 3, 5, 3, 1, 3, 5, 1, 2, 3, 4, 5, 1, 1, 2, 3, 5, 1, 2, 3, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 3, 5, 1, 1, 1, 3, 5, 1, 3, 5, 1, 2, 3, 5], \"Freq\": [0.08106077281954023, 0.32071870985122436, 0.5533278840290354, 0.042292577123238376, 0.0035243814269365317, 0.09934307392498401, 0.6495508679710493, 0.2445367973538068, 0.3104772557033812, 0.6209545114067624, 0.931859135712957, 0.06656136683663978, 0.15567805932198067, 0.6227122372879227, 0.4110680450074731, 0.17838801953154493, 0.3722880407614851, 0.03878000424598803, 0.8662716446990153, 0.06794287409404041, 0.050957155570530314, 0.20284872494150502, 0.6761624164716834, 0.12020665181718816, 0.07976696766674449, 0.9173201281675616, 0.9741520128739287, 0.11379693291341869, 0.28069910118643276, 0.5120861981103841, 0.015172924388455826, 0.07586462194227912, 0.09547269211724324, 0.5114608506280888, 0.30687651037685326, 0.061375302075370654, 0.013638956016749035, 0.042429800654889424, 0.01697192026195577, 0.3394384052391154, 0.025457880392933657, 0.5770452889064962, 0.17359077485976449, 0.8100902826789009, 0.9828329191616169, 0.11618036111684787, 0.6555891805879274, 0.08298597222631991, 0.14937475000737585, 0.00829859722263199, 0.1387232432642476, 0.016320381560499718, 0.1958445787259966, 0.6446550716397388, 0.08404541704941526, 0.8964844485270961, 0.9517883280678036, 0.032264011120942494, 0.9722526172035441, 0.9741576821733926, 0.31242423253481855, 0.7029545232033417, 0.11224261396642804, 0.02806065349160701, 0.8418196047482103, 0.9793120937505326, 0.3542515723193227, 0.1807405981221034, 0.2168887177465241, 0.04337774354930482, 0.20242946989675584, 0.32102701409429185, 0.195134067390648, 0.15107153604437265, 0.012589294670364386, 0.32102701409429185, 0.29793799504052415, 0.049656332506754025, 0.6455323225878024, 0.15601254696920858, 0.11556484960682117, 0.4795941258683079, 0.24268618417432447, 0.011556484960682118, 0.07500202690406176, 0.07500202690406176, 0.7750209446753048, 0.07500202690406176, 0.02709608981099208, 0.9212670535737306, 0.05419217962198416, 0.9220712255888408, 0.041912328435856405, 0.97445778332462, 0.2077317783751729, 0.07331709825006102, 0.70873194975059, 0.17252676018258617, 0.8051248808520688, 0.014488654645459153, 0.3428981599425333, 0.24147757742431922, 0.31875040220010137, 0.08693192787275492, 0.16766647206976912, 0.06209869335917375, 0.149036864062017, 0.6209869335917375, 0.9591397518837912, 0.18329114014668083, 0.7331645605867233, 0.050229070357309176, 0.853894196074256, 0.050229070357309176, 0.5684622112247795, 0.12870842518296893, 0.2359654461687764, 0.06435421259148447, 0.9441377745427899, 0.9899884793424482, 0.07594245375893514, 0.07594245375893514, 0.8353669913482865, 0.9486516520414785, 0.049929034317972555, 0.015013348863898612, 0.030026697727797223, 0.10509344204729028, 0.8407475363783222, 0.13456615346355297, 0.11136509252156108, 0.6821111916945616, 0.009280424376796756, 0.06496297063757729, 0.1660598980793289, 0.711685277482838, 0.1423370554965676, 0.14024206905238698, 0.14024206905238698, 0.6661498279988382, 0.07012103452619349, 0.493750604205466, 0.020572941841894416, 0.06171882552568325, 0.34974001131220506, 0.07200529644663045, 0.8750412258963162, 0.08627167015879172, 0.012324524308398819, 0.012324524308398819, 0.6159925800329843, 0.13588071618374656, 0.17211557383274562, 0.06341100088574839, 0.9615013545113418, 0.047994720694905325, 0.5359410477597761, 0.2639709638219793, 0.007999120115817555, 0.14398416208471598, 0.11933051327790056, 0.06508937087885486, 0.2440851407957057, 0.010848228479809142, 0.5532596524702662, 0.008995269372571666, 0.08995269372571667, 0.8995269372571667, 0.9569940311736053, 0.013488034277950488, 0.9576504337344847, 0.013488034277950488, 0.9844671379165955, 0.028735500560686134, 0.4741357592513212, 0.31609050616754747, 0.13649362766325912, 0.050287125981200734, 0.904632387389116, 0.17422042113974454, 0.653326579274042, 0.1306653158548084, 0.9794722110286871, 0.07806259970891657, 0.15612519941783315, 0.7025633973802491, 0.9137510864054986, 0.0268750319531029, 0.0268750319531029, 0.013041486873226442, 0.7303232649006808, 0.2477882505913024, 0.018857118229401656, 0.03771423645880331, 0.9239987932406812, 0.027050905328705938, 0.09467816865047078, 0.852103517854237, 0.027050905328705938, 0.14245818355396941, 0.8547491013238164, 0.03764305042663345, 0.6305210946461102, 0.3011444034130676, 0.028232287819975083, 0.8851268726106314, 0.07376057271755261, 0.9110037355926935, 0.06073358237284624, 0.15456915237157576, 0.206092203162101, 0.5925150840910404, 0.025761525395262625, 0.9725476529892214, 0.9187787234136129, 0.6288039748562176, 0.05154130941444407, 0.2680148089551092, 0.05154130941444407, 0.9517514675357247, 0.17205916261253892, 0.34411832522507785, 0.4779421183681637, 0.12619830137034957, 0.21586551550191377, 0.2623596265330952, 0.06642015861597346, 0.32877978514906864, 0.9517083431656481, 0.0325428132601971, 0.9600129911758144, 0.9844669617265106, 0.006441745930190967, 0.3349707883699303, 0.34141253430012125, 0.3156455505793574, 0.9300856341371136, 0.9470150100301966, 0.034560484036426045, 0.8442632528898362, 0.11355587611968557, 0.004937212005203721, 0.04050981267834454, 0.008101962535668908, 0.04861177521401345, 0.8912158789235799, 0.3816612557959097, 0.02848218326835147, 0.02848218326835147, 0.017089309961010882, 0.541161482098678, 0.0719841055322082, 0.057587284425766556, 0.10077774774509148, 0.7774283397478485, 0.9280434092104947, 0.029936884168080474, 0.029936884168080474, 0.07344524213737737, 0.11541395193016445, 0.7869133086147576, 0.010492177448196768, 0.9046384954430748, 0.01798844058543954, 0.971375791613735, 0.1679037213356389, 0.08395186066781946, 0.6716148853425556, 0.9528564190348683, 0.035271986234528256, 0.07054397246905651, 0.07054397246905651, 0.705439724690565, 0.07054397246905651, 0.034413365022326496, 0.8947474905804889, 0.034413365022326496, 0.9301097325988832, 0.903840134074628, 0.06456000957675914, 0.921169012209175, 0.07369352097673401, 0.04107148102837795, 0.010267870257094488, 0.05133935128547244, 0.8933047123672205, 0.9566329982726351, 0.03751501954010334, 0.02632810585402296, 0.3159372702482755, 0.5792183287885051, 0.05265621170804592, 0.28620853330565205, 0.6869004799335648, 0.03185624933933858, 0.1592812466966929, 0.7964062334834645, 0.9256464688410124, 0.022000709153494443, 0.9240297844467665, 0.05500177288373611, 0.05506896930525455, 0.1982482894989164, 0.011013793861050912, 0.7379241886904111, 0.8647201502415647, 0.12810668892467625, 0.9424926839428658, 0.053348642487332024, 0.8697654819600588, 0.05436034262250367, 0.8921305307952988, 0.07757656789524338, 0.9718741368081797, 0.9517175033513686, 0.8368679514662425, 0.10460849393328031, 0.052304246966640155, 0.9578186875390573, 0.8561345703469033, 0.05632464278598048, 0.022529857114392194, 0.011264928557196097, 0.04505971422878439, 0.9502093122757881, 0.03276583835433752, 0.9809865985032874, 0.03546487060514116, 0.18239076311215455, 0.42051203717524516, 0.3445158858785141, 0.01519923025934621, 0.023300705861651477, 0.9553289403277105, 0.022032321715268772, 0.15422625200688142, 0.8151959034649446, 0.22282559788802897, 0.7639734784732422, 0.05284141426911352, 0.05284141426911352, 0.2717558448125838, 0.6114506508283136, 0.8983741089627967, 0.07486450908023305, 0.2218794545397752, 0.21535358822978182, 0.15988372459483802, 0.013051732619986777, 0.38828904544460663, 0.19243232880563904, 0.13745166343259932, 0.6666405676481066, 0.2770769796502793, 0.49582196358471026, 0.2150992342021905, 0.01093724919672155, 0.6849124155670554, 0.13045950772705817, 0.179381823124705, 0.9747153465437547, 0.028045062070443015, 0.9535321103950625, 0.9159162950935636, 0.26368825673269264, 0.030135800769450587, 0.14314505365489028, 0.20341665519379146, 0.3540956590410444, 0.2980652757896314, 0.49677545964938563, 0.19871018385975425, 0.9730614541741517, 0.9768462100947973, 0.24174573038717403, 0.30560309313095585, 0.30560309313095585, 0.013683720587953247, 0.13227596568354805, 0.7836299372662819, 0.139311988847339, 0.05224199581775213, 0.9256513114741085, 0.9410965765066305, 0.026888473614475154, 0.9578246004474824, 0.37405980330450667, 0.06801087332809212, 0.32116245738265725, 0.02267029110936404, 0.21158938368739771, 0.9741549473904443, 0.2891470555610983, 0.012946883084825297, 0.479034674138536, 0.017262510779767065, 0.20715012935720475, 0.9860063417652583, 0.07951482530703287, 0.8746630783773616, 0.13728531506930836, 0.5834625890445605, 0.20592797260396253, 0.06864265753465418, 0.008580332191831772, 0.9469956892201588, 0.8190694719147278, 0.02978434443326283, 0.13402954994968275, 0.8991178365429475, 0.10577856900505266, 0.012609753805565666, 0.7313657207228087, 0.20175606088905065, 0.037829261416697, 0.007414033808480258, 0.5338104342105785, 0.1927648790204867, 0.17052277759504592, 0.09638243951024335, 0.029005032294647706, 0.11602012917859082, 0.8411459365447835, 0.048025701676452544, 0.9365011826908246, 0.9185247370034232, 0.04142761769272885, 0.7456971184691193, 0.11599732953964079, 0.04971314123127463, 0.04971314123127463, 0.14537888982903394, 0.03028726871438207, 0.5330559293731244, 0.27864287217231504, 0.018172361228629242, 0.7029720983022669, 0.01562160218449482, 0.2187024305829275, 0.04686480655348446, 0.9434523369838721, 0.04101966682538574, 0.9768431741031636, 0.007590279912073775, 0.9943266684816645, 0.9838088129603832, 0.055844930621414414, 0.10052087511854595, 0.022337972248565766, 0.8153359870726504, 0.11069865076087963, 0.04744227889751984, 0.18449775126813273, 0.031628185931679896, 0.6272923543116512, 0.918771678234257, 0.9855314101504207, 0.9639411711420364, 0.042119978663997276, 0.4001397973079741, 0.5264997332999659, 0.023311842609143686, 0.9557855469748912, 0.740845232747738, 0.01683739165335768, 0.1683739165335768, 0.06734956661343072, 0.32358901133774515, 0.006222865602648946, 0.23646889290065992, 0.018668596807946837, 0.4107091297748304, 0.9790896890107262, 0.08232777219122292, 0.07409499497210063, 0.197586653258935, 0.024698331657366877, 0.617458291434172, 0.8819263692501329, 0.1075519962500162, 0.04955522463670325, 0.2044153016264009, 0.22919291394475252, 0.012388806159175812, 0.5079410525262082, 0.9730763360115828, 0.9517097050073366, 0.9858883092379253, 0.15658009399017686, 0.059246522050337196, 0.18197146058317853, 0.008463788864333886, 0.5924652205033719, 0.35096891985746403, 0.006051188273404552, 0.042358317913831864, 0.5990676390670506, 0.19699973291830128, 0.1641664440985844, 0.6238324875746207, 0.039114605560096584, 0.3285626867048113, 0.320739765592792, 0.30509392336875335, 0.9046428264865378, 0.14717370237100866, 0.14717370237100866, 0.13168173370037617, 0.023237953005948737, 0.5499648878074535, 0.011780964110340745, 0.04712385644136298, 0.9424771288272595, 0.016719173731405024, 0.05015752119421507, 0.9195545552272764, 0.9831898306255165, 0.9338892717019857, 0.049152066931683454, 0.08196570731822697, 0.08909316012850758, 0.3278628292729079, 0.4989216967196425, 0.03510906224012338, 0.7841023900294223, 0.05851510373353898, 0.11703020746707796, 0.24165871186106438, 0.604146779652661, 0.06693917081082616, 0.9371483913515662, 0.00973284346691399, 0.7494289469523772, 0.22385539973902174, 0.01946568693382798, 0.020393116116824605, 0.050982790292061514, 0.7851349704977473, 0.030589674175236906, 0.11216213864253533, 0.07045682876017785, 0.3029643636687647, 0.6059287273375294, 0.021137048628053354, 0.016198968892362393, 0.2591835022777983, 0.696555662371583, 0.016198968892362393, 0.9725472889512966, 0.04146839886652825, 0.9537731739301497, 0.0231900480307821, 0.9507919692620661, 0.05656267726149072, 0.0899860774614625, 0.7713092353839643, 0.010284123138452858, 0.07455989275378322, 0.19644035035308863, 0.6449791503259744, 0.16042628612168905, 0.9854327180416818, 0.12050210568717734, 0.41712267353253696, 0.4217573699051207, 0.03244287460808621, 0.009269392745167488, 0.17518955760064903, 0.8133800888601562, 0.1932689605654476, 0.29990011122224625, 0.05331557532839933, 0.45318239029139434, 0.9439403116922012, 0.9868209839771619, 0.05792764243120745, 0.28963821215603724, 0.5985856384558104, 0.05792764243120745, 0.18259469515005305, 0.06351119831306194, 0.23022809388484952, 0.5239673860827609, 0.8822560731592094, 0.025948708034094395, 0.07784612410228318, 0.9791220667782528, 0.9741526684057141, 0.9634725739339253, 0.9890743853999008, 0.9528265117236379, 0.05505003614258294, 0.8074005300912165, 0.13762509035645737, 0.011304402500600212, 0.13565283000720255, 0.4936255758595426, 0.026376939168067163, 0.32782767251740613, 0.10970901136167108, 0.6682276146574512, 0.14960319731136965, 0.05984127892454786, 0.019947092974849288, 0.7625525700149545, 0.14919506804640412, 0.08288614891466896, 0.9352043405511175, 0.0381716057367803, 0.01908580286839015, 0.917939209762181, 0.9791471645133362, 0.925674821329492, 0.377283085633412, 0.11690461808359245, 0.14878769574275405, 0.09564923297748475, 0.2603784675498196, 0.8124890662796305, 0.15089082659478853, 0.03482095998341274, 0.061594272460057685, 0.7760878329967268, 0.08623198144408076, 0.024637708984023076, 0.04927541796804615, 0.9723968908369179, 0.03889587563347672, 0.6813047703714905, 0.03028021201651069, 0.15140106008255344, 0.015140106008255345, 0.10598074205778742, 0.9860618477954786, 0.8442638082787403, 0.05116750353204486, 0.02558375176602243, 0.0767512552980673, 0.021357500171738004, 0.9610875077282102, 0.021561717723572172, 0.19405545951214953, 0.4527960721950156, 0.32342576585358257, 0.04775774125325326, 0.9312759544384386, 0.02387887062662663, 0.8413699518855187, 0.15297635488827613, 0.32392853141254313, 0.6478570628250863, 0.98600756154509, 0.016778246177388136, 0.10066947706432881, 0.8624018535177502, 0.01342259694191051, 0.010066947706432882, 0.9470315267248157, 0.023545766014924665, 0.70637298044774, 0.08241018105223633, 0.18836612811939732, 0.02135357648387658, 0.5893587109549936, 0.3886350920065538, 0.01841990248289804, 0.884155319179106, 0.0920995124144902, 0.07628658406242181, 0.20706358531228777, 0.6974773399992851, 0.01089808343748883, 0.024737563808353954, 0.3413783805552845, 0.5640164548304701, 0.06926517866339106, 0.9832503707506132, 0.8841658790425615, 0.11052073488032019, 0.04944613676342228, 0.9394765985050233, 0.259706771336907, 0.6752376054759582, 0.9612679176444981, 0.042723018561977695, 0.9464887911259544, 0.9526529003928699, 0.02721865429693914, 0.011342752605708703, 0.29491156774842625, 0.3743108359883872, 0.29491156774842625, 0.022685505211417406, 0.9750809084793693, 0.9768403988953387, 0.07644785055488128, 0.716698598952012, 0.20067560770656337, 0.00955598131936016, 0.9116447680965697, 0.07597039734138081, 0.043126404871509115, 0.043126404871509115, 0.23719522679330013, 0.539080060893864, 0.12937921461452734, 0.8971779567590088, 0.04600912598764148, 0.04600912598764148, 0.01150228149691037, 0.04944612830035654, 0.9394764377067742, 0.3640015683070265, 0.04254563785406803, 0.21272818927034015, 0.07563668951834317, 0.30254675807337267, 0.9118002475876283, 0.045590012379381414, 0.5259217089368754, 0.11027390671257065, 0.2205478134251413, 0.0424130410432964, 0.11027390671257065, 0.9486184299183283, 0.02496364289258759, 0.044575428375139524, 0.9583717100654997, 0.9741590756959425, 0.06429667747593427, 0.0025214383323895794, 0.4815947214864097, 0.010085753329558318, 0.44251242733437124, 0.02658729251085041, 0.02658729251085041, 0.9305552378797642, 0.0867686380769123, 0.150398972666648, 0.6073804665383862, 0.15618354853844216, 0.8387046340123855, 0.13479181618056194, 0.029953736929013767, 0.45007892654988996, 0.16366506419996, 0.261864102719936, 0.016366506419996, 0.11456554493997198, 0.20609997705509386, 0.535859940343244, 0.22258797521950138, 0.03297599632881502, 0.9591553245263702, 0.01934022035876505, 0.9670110179382525, 0.019539440262370554, 0.39078880524741105, 0.3614796448538552, 0.18562468249252026, 0.03907888052474111, 0.6389665077890989, 0.2513966588022684, 0.0837988862674228, 0.0209497215668557, 0.4342517923933947, 0.2617408063741009, 0.17845964070961426, 0.029743273451602374, 0.0951784750451276, 0.008075401576004762, 0.29071445673617147, 0.1292064252160762, 0.5006748977122952, 0.0646032126080381, 0.07162617253160455, 0.9311402429108592, 0.9858899031337288, 0.9768477215452345, 0.8252089287645267, 0.16504178575290535, 0.9582901527795734, 0.15040708577895098, 0.8272389717842304, 0.037601771444737746, 0.17354526776314316, 0.23139369035085758, 0.13980035458697646, 0.24585579599778618, 0.21211088282161944, 0.9688069165035886, 0.9634718147843124, 0.9877399224950686, 0.9741594027809158, 0.9730856837573975, 0.02025827781763461, 0.5449476732943711, 0.4071913841344557, 0.024309933381161535, 0.0020258277817634613, 0.4602385380527853, 0.3825359277321852, 0.12850816322253097, 0.029885619354076968, 0.29859720022143615, 0.7025816475798498, 0.22456631313932546, 0.03962934937752802, 0.26419566251685345, 0.42271306002696557, 0.052839132503370696, 0.24792201403646297, 0.6941816393020963, 0.10361062324280071, 0.870329235239526, 0.9155906582008523, 0.0370376150792339, 0.9259403769808474, 0.06942763081683233, 0.6109631511881245, 0.2568822340222796, 0.0416565784900994, 0.0208282892450497, 0.20515650877314176, 0.7522405321681864, 0.3744959386914299, 0.3517992151343735, 0.23831559734909175, 0.034045085335584535, 0.9416918597765398, 0.03553554187835999, 0.8635719018658129, 0.10159669433715446, 0.016932782389525745, 0.9633156388043337, 0.9401648607986162, 0.1467583142627452, 0.04193094693221291, 0.12579284079663872, 0.010482736733053227, 0.6708951509154065, 0.158120659682512, 0.05270688656083733, 0.22588665668930283, 0.037647776114883805, 0.5270688656083733, 0.036049448080320234, 0.05046922731244833, 0.05767911692851238, 0.021629668848192143, 0.8291373058473654, 0.9517258864442071, 0.9688002292594627, 0.9887573654479928, 0.3781991752776046, 0.5943129897219501, 0.33209974368551276, 0.4749026334702832, 0.1693708692796115, 0.019925984621130766, 0.2229296090850659, 0.5162580420917315, 0.1407976478431995, 0.06648777814817755, 0.05084359505448871, 0.022969225531212695, 0.8957997957172952, 0.04593845106242539, 0.022969225531212695, 0.9491411756060583, 0.060918971565837574, 0.12183794313167515, 0.8122529542111677, 0.01771002916066133, 0.9386315455150505, 0.01771002916066133, 0.10888282802069534, 0.7621797961448673, 0.0068051767512934585, 0.12249318152328224, 0.017283497863414363, 0.05185049359024309, 0.9333088846243756, 0.2402202463085822, 0.43732403815152143, 0.04311645446564296, 0.04927594796073481, 0.2279012593183985, 0.27104302401460656, 0.1025568198974187, 0.2197646140658972, 0.04029017924541449, 0.36627435677649534, 0.9666110806438104, 0.24360344959163416, 0.1059145433007105, 0.5401641708336236, 0.11121027046574603, 0.9570291314565128, 0.13240572769611628, 0.13240572769611628, 0.7355873760895348, 0.09366222854621183, 0.08429600569159064, 0.768030274078937, 0.028098668563863546, 0.028098668563863546, 0.9741593350849018, 0.03639508390049879, 0.018197541950249394, 0.03639508390049879, 0.8916795555622203, 0.5725701122719186, 0.02290280449087674, 0.1488682291906988, 0.25193084939964416, 0.9688104492074929, 0.2503359005466261, 0.34661893921840536, 0.23878193590601257, 0.011553964640613512, 0.15405286187484682, 0.11726942414147379, 0.2501747715018107, 0.46125973495646355, 0.007817961609431585, 0.1602682129933475, 0.024098589710794014, 0.09639435884317606, 0.8675492295885845, 0.9713210155053538, 0.9740997821452441, 0.04939817702106705, 0.04939817702106705, 0.8891671863792069, 0.06418903495633788, 0.8665519719105613, 0.06418903495633788, 0.05912867260196984, 0.0739108407524623, 0.2143414381821407, 0.6504153986216683], \"Term\": [\"000\", \"000\", \"000\", \"000\", \"000\", \"1960\", \"1960\", \"1960\", \"63\", \"63\", \"academic\", \"academic\", \"accommodate\", \"accommodate\", \"act\", \"act\", \"act\", \"act\", \"activities\", \"activities\", \"activities\", \"administration\", \"administration\", \"administration\", \"alexander\", \"alexander\", \"amateur\", \"american\", \"american\", \"american\", \"american\", \"american\", \"area\", \"area\", \"area\", \"area\", \"area\", \"asked\", \"asked\", \"asked\", \"asked\", \"asked\", \"assessment\", \"assessment\", \"aug\", \"available\", \"available\", \"available\", \"available\", \"available\", \"away\", \"away\", \"away\", \"away\", \"baby\", \"baby\", \"ball\", \"ball\", \"baseball\", \"bat\", \"battery\", \"battery\", \"bed\", \"bed\", \"bed\", \"berlin\", \"best\", \"best\", \"best\", \"best\", \"best\", \"better\", \"better\", \"better\", \"better\", \"better\", \"blocks\", \"blocks\", \"blocks\", \"board\", \"board\", \"board\", \"board\", \"board\", \"boat\", \"boat\", \"boat\", \"boat\", \"boats\", \"boats\", \"boats\", \"bob\", \"bob\", \"bombs\", \"boy\", \"boy\", \"boy\", \"bridge\", \"bridge\", \"business\", \"business\", \"business\", \"business\", \"business\", \"came\", \"came\", \"came\", \"came\", \"castro\", \"catastrophe\", \"catastrophe\", \"cattle\", \"cattle\", \"cattle\", \"center\", \"center\", \"center\", \"center\", \"charlie\", \"charter\", \"chemical\", \"chemical\", \"chemical\", \"chest\", \"chest\", \"child\", \"child\", \"child\", \"child\", \"city\", \"city\", \"city\", \"city\", \"city\", \"claim\", \"claim\", \"claim\", \"claims\", \"claims\", \"claims\", \"claims\", \"class\", \"class\", \"class\", \"class\", \"class\", \"clay\", \"clay\", \"clay\", \"clay\", \"club\", \"club\", \"club\", \"club\", \"coach\", \"college\", \"college\", \"college\", \"college\", \"college\", \"come\", \"come\", \"come\", \"come\", \"come\", \"committee\", \"committee\", \"committee\", \"communism\", \"communist\", \"communist\", \"communist\", \"communists\", \"company\", \"company\", \"company\", \"company\", \"company\", \"comprehensive\", \"concrete\", \"concrete\", \"concrete\", \"congo\", \"conversion\", \"conversion\", \"conversion\", \"cooling\", \"cooling\", \"cooling\", \"corps\", \"corps\", \"corps\", \"couldn\", \"couldn\", \"couldn\", \"council\", \"council\", \"council\", \"council\", \"county\", \"county\", \"court\", \"court\", \"court\", \"court\", \"coverage\", \"coverage\", \"creative\", \"creative\", \"credit\", \"credit\", \"credit\", \"credit\", \"cuba\", \"curriculum\", \"cut\", \"cut\", \"cut\", \"cut\", \"dad\", \"daily\", \"daily\", \"daily\", \"day\", \"day\", \"day\", \"day\", \"day\", \"dear\", \"democratic\", \"democratic\", \"democrats\", \"department\", \"department\", \"department\", \"department\", \"depreciation\", \"designers\", \"development\", \"development\", \"development\", \"development\", \"didn\", \"didn\", \"didn\", \"didn\", \"don\", \"don\", \"don\", \"don\", \"don\", \"door\", \"door\", \"door\", \"door\", \"drill\", \"drill\", \"drill\", \"east\", \"east\", \"east\", \"east\", \"educated\", \"election\", \"election\", \"electricity\", \"electricity\", \"electricity\", \"employee\", \"engine\", \"engine\", \"engine\", \"engine\", \"engine\", \"engineers\", \"engineers\", \"engineers\", \"evaluation\", \"exceed\", \"exceed\", \"expenditures\", \"expenditures\", \"eyes\", \"eyes\", \"eyes\", \"eyes\", \"faculty\", \"faculty\", \"farm\", \"farm\", \"farm\", \"farm\", \"farmers\", \"farmers\", \"father\", \"father\", \"father\", \"favorite\", \"feed\", \"feed\", \"feed\", \"felt\", \"felt\", \"felt\", \"felt\", \"fig\", \"fig\", \"fiscal\", \"fiscal\", \"fishing\", \"fishing\", \"football\", \"football\", \"forests\", \"fountain\", \"frame\", \"frame\", \"frame\", \"gallery\", \"game\", \"game\", \"game\", \"game\", \"game\", \"games\", \"games\", \"gen\", \"general\", \"general\", \"general\", \"general\", \"general\", \"georgia\", \"georgia\", \"girl\", \"girl\", \"girl\", \"god\", \"god\", \"going\", \"going\", \"going\", \"going\", \"golf\", \"golf\", \"good\", \"good\", \"good\", \"good\", \"good\", \"got\", \"got\", \"got\", \"government\", \"government\", \"government\", \"government\", \"green\", \"green\", \"green\", \"hadn\", \"hair\", \"hair\", \"handsome\", \"head\", \"head\", \"head\", \"head\", \"head\", \"hearing\", \"hearing\", \"hearing\", \"heel\", \"hell\", \"high\", \"high\", \"high\", \"high\", \"high\", \"hit\", \"hit\", \"hit\", \"hits\", \"hole\", \"hole\", \"holes\", \"home\", \"home\", \"home\", \"home\", \"home\", \"homer\", \"house\", \"house\", \"house\", \"house\", \"house\", \"hughes\", \"import\", \"import\", \"important\", \"important\", \"important\", \"important\", \"important\", \"imports\", \"inch\", \"inch\", \"inch\", \"india\", \"india\", \"industrial\", \"industrial\", \"industrial\", \"industrial\", \"interest\", \"interest\", \"interest\", \"interest\", \"interest\", \"interference\", \"interference\", \"interference\", \"interior\", \"interior\", \"inventory\", \"island\", \"island\", \"island\", \"island\", \"island\", \"john\", \"john\", \"john\", \"john\", \"john\", \"junior\", \"junior\", \"junior\", \"junior\", \"jury\", \"jury\", \"katanga\", \"kennedy\", \"kennedy\", \"khrushchev\", \"knew\", \"knew\", \"knew\", \"knew\", \"know\", \"know\", \"know\", \"know\", \"know\", \"laboratories\", \"laos\", \"laughed\", \"laws\", \"laws\", \"laws\", \"leader\", \"leader\", \"league\", \"league\", \"league\", \"league\", \"left\", \"left\", \"left\", \"left\", \"left\", \"leg\", \"let\", \"let\", \"let\", \"let\", \"let\", \"library\", \"library\", \"life\", \"life\", \"life\", \"life\", \"life\", \"liked\", \"lips\", \"listening\", \"little\", \"little\", \"little\", \"little\", \"little\", \"ll\", \"ll\", \"ll\", \"ll\", \"loans\", \"loans\", \"loans\", \"local\", \"local\", \"local\", \"local\", \"locations\", \"look\", \"look\", \"look\", \"look\", \"look\", \"looked\", \"looked\", \"looked\", \"love\", \"love\", \"love\", \"loved\", \"maintain\", \"maintain\", \"man\", \"man\", \"man\", \"man\", \"management\", \"management\", \"management\", \"management\", \"manufactured\", \"manufactured\", \"mayor\", \"mayor\", \"medical\", \"medical\", \"medical\", \"medical\", \"meeting\", \"meeting\", \"meeting\", \"meeting\", \"meeting\", \"members\", \"members\", \"members\", \"members\", \"military\", \"military\", \"military\", \"military\", \"moscow\", \"mother\", \"mother\", \"motors\", \"motors\", \"mr\", \"mr\", \"mr\", \"mr\", \"mr\", \"mrs\", \"mrs\", \"mrs\", \"musical\", \"national\", \"national\", \"national\", \"national\", \"national\", \"nations\", \"nations\", \"night\", \"night\", \"night\", \"night\", \"nodded\", \"nuclear\", \"officer\", \"officer\", \"officer\", \"officer\", \"old\", \"old\", \"old\", \"old\", \"organizations\", \"organizations\", \"organizations\", \"paint\", \"paintings\", \"pale\", \"panels\", \"participation\", \"party\", \"party\", \"party\", \"people\", \"people\", \"people\", \"people\", \"people\", \"period\", \"period\", \"period\", \"period\", \"period\", \"piece\", \"piece\", \"piece\", \"pieces\", \"pieces\", \"pieces\", \"pitcher\", \"pitching\", \"pittsburgh\", \"place\", \"place\", \"place\", \"place\", \"place\", \"planning\", \"planning\", \"planning\", \"plant\", \"plant\", \"plant\", \"plant\", \"plant\", \"plastic\", \"plastic\", \"play\", \"play\", \"play\", \"play\", \"play\", \"player\", \"playing\", \"playing\", \"playing\", \"playing\", \"police\", \"police\", \"policy\", \"policy\", \"policy\", \"policy\", \"political\", \"political\", \"political\", \"pool\", \"pool\", \"pound\", \"pound\", \"premier\", \"president\", \"president\", \"president\", \"president\", \"president\", \"producers\", \"production\", \"production\", \"production\", \"production\", \"program\", \"program\", \"program\", \"projects\", \"projects\", \"projects\", \"property\", \"property\", \"property\", \"property\", \"public\", \"public\", \"public\", \"public\", \"publications\", \"published\", \"published\", \"puerto\", \"puerto\", \"ratio\", \"ratio\", \"rayburn\", \"rayburn\", \"recommendation\", \"recreation\", \"recreation\", \"report\", \"report\", \"report\", \"report\", \"report\", \"republican\", \"republicans\", \"research\", \"research\", \"research\", \"research\", \"resources\", \"resources\", \"respect\", \"respect\", \"respect\", \"respect\", \"respect\", \"rhode\", \"rhode\", \"rhode\", \"rhode\", \"rico\", \"rico\", \"right\", \"right\", \"right\", \"right\", \"right\", \"roy\", \"roy\", \"run\", \"run\", \"run\", \"run\", \"run\", \"runs\", \"runs\", \"russia\", \"russia\", \"ruth\", \"said\", \"said\", \"said\", \"said\", \"said\", \"sat\", \"sat\", \"sat\", \"school\", \"school\", \"school\", \"school\", \"season\", \"season\", \"season\", \"second\", \"second\", \"second\", \"second\", \"second\", \"secretary\", \"secretary\", \"secretary\", \"secretary\", \"sen\", \"senate\", \"senate\", \"service\", \"service\", \"service\", \"service\", \"service\", \"services\", \"services\", \"services\", \"services\", \"set\", \"set\", \"set\", \"set\", \"set\", \"shall\", \"shall\", \"shall\", \"shall\", \"shall\", \"shelter\", \"shelter\", \"shoes\", \"sick\", \"signs\", \"signs\", \"silent\", \"site\", \"site\", \"site\", \"small\", \"small\", \"small\", \"small\", \"small\", \"smile\", \"smiled\", \"soviet\", \"stadium\", \"stared\", \"state\", \"state\", \"state\", \"state\", \"state\", \"states\", \"states\", \"states\", \"states\", \"stations\", \"stations\", \"stock\", \"stock\", \"stock\", \"stock\", \"stock\", \"stockholders\", \"stockholders\", \"stood\", \"stood\", \"strange\", \"susan\", \"susan\", \"system\", \"system\", \"system\", \"system\", \"system\", \"tangible\", \"tangible\", \"tax\", \"tax\", \"tax\", \"tax\", \"team\", \"team\", \"technical\", \"technical\", \"technical\", \"textile\", \"thereof\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"think\", \"think\", \"think\", \"think\", \"think\", \"thought\", \"thought\", \"thought\", \"thought\", \"thought\", \"till\", \"tongue\", \"troops\", \"trust\", \"trust\", \"united\", \"united\", \"united\", \"united\", \"use\", \"use\", \"use\", \"use\", \"use\", \"vehicles\", \"vehicles\", \"vehicles\", \"vehicles\", \"vernon\", \"voice\", \"voice\", \"voice\", \"vote\", \"vote\", \"vote\", \"war\", \"war\", \"war\", \"war\", \"wasn\", \"wasn\", \"wasn\", \"water\", \"water\", \"water\", \"water\", \"water\", \"way\", \"way\", \"way\", \"way\", \"way\", \"weapons\", \"week\", \"week\", \"week\", \"week\", \"welfare\", \"went\", \"went\", \"went\", \"west\", \"west\", \"west\", \"west\", \"west\", \"willie\", \"woman\", \"woman\", \"woman\", \"woman\", \"won\", \"won\", \"won\", \"won\", \"wondered\", \"work\", \"work\", \"work\", \"work\", \"work\", \"world\", \"world\", \"world\", \"world\", \"world\", \"wouldn\", \"wouldn\", \"wouldn\", \"yankees\", \"yards\", \"yes\", \"yes\", \"yes\", \"yesterday\", \"yesterday\", \"yesterday\", \"young\", \"young\", \"young\", \"young\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 2, 3, 4, 5]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el195162100303617088544540424\", ldavis_el195162100303617088544540424_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el195162100303617088544540424\", ldavis_el195162100303617088544540424_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el195162100303617088544540424\", ldavis_el195162100303617088544540424_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d14c87",
   "metadata": {},
   "source": [
    "Q: What conclusions do you draw from the visualization above? Please address the principal component scatterplot and the salient terms graph.\n",
    "\n",
    "A: The vizualization above includes a number of charts, the first being an Intertopic Distance Map. This plot shows the spatial relationship between the topics. Each topic is represented as a circle, and the distance between circles indicate the similarity or differnce between topics. For example Topic 2 and 4 are almost entirely overlapping so this indicates they have very similar topics. Topic 3 has some similarities with Topic 2 as they overlap a little. then there is Topic 1 and Topic 5 that show they are different as they are further removed from all topics with no overlap at all. As you hover over these circles they will highlight the corresponding Top 30 Salient Terms. Since Topic 2 and Topic 4 stongly overlap, it might be worth adjusting the number of topics or doing additional preprocessing.\n",
    "\n",
    "The Topic-Word Relevance Bar Charts shows the most relevant words associated with that topic. The relevance of each word is represented by the length of the bar. The relevance metric (λ) is adjustable and allows the focus to be shifted between words that are highly probable within the topic. Adjusting the relevance metric helps to fine-tune the focus between common words and distinctive words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2f7be1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
